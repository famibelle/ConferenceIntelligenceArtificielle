# <div style="display: flex; justify-content: center; align-items: center; height: 100vh; text-align: center; font-size: 10vw; font-weight: bold; width: 100%;">L'Intelligence Artificielle</div>



---

Maitre Corbeau sur un arbre ?

---

Maitre Corbeau sur un arbre perch√©
http://andreetgyps.a.n.pic.centerblog.net/o/6b0e0247.jpg

---
# Tokenisation

Les tokens en traitement du langage naturel (NLP) sont comme les syllabes en po√©sie. Tout comme les syllabes sont les √©l√©ments constitutifs du rythme et de la structure d'un po√®me, les tokens sont les unit√©s fondamentales qui permettent aux mod√®les d'IA de traiter et de comprendre le texte.

## "Maitre Corbeau sur un arbre perch√©" ‚Üí d√©casyllabe
- **Syllabes dans un po√®me :** Maitre Corbeau sur un arbre perch√©.
- **Tokens en NLP :** [Mai ##tre Cor ##beau sur un ar### bre perch√©.].

---

# Token dans les Mod√®les d'IA

La limite de tokens d√©finit le nombre maximum de tokens qu'un mod√®le peut traiter dans une seule entr√©e. Des limites de tokens plus √©lev√©es permettent de g√©rer des contextes plus longs, rendant les mod√®les plus efficaces pour des t√¢ches comme la synth√®se, l'analyse de code et la g√©n√©ration de documents.

| Mod√®le         | Taille Max (tokens) | Pages Livre de Poche Approx. |
|----------------|---------------------|------------------------------|
| GPT-5          | 128 000             | ~512                         |
| Llama 3.1      | 128 000             | ~512                         |
| Mistral Large  | 64 000              | ~256                         |

---

# Embedding

## Transformer les Tokens en Repr√©sentations Num√©riques

<div style="display: flex; align-items: center; gap: 20px;">

    <div style="flex: 1;">

L'embedding transforme les tokens en vecteurs, qui servent de v√©ritables points d'entr√©e pour le LLM.

    </div>

    <div style="flex: 1;">

![Exemple d'Embedding](https://causewriter.ai/wp-content/uploads/2023/08/image-2.png)

    </div>
</div>

---

# Comment la Tokenisation et l'Embedding Fonctionnent Ensemble :
**Tokenisation :**
- Divise le texte en tokens (par exemple, mots, sous-mots ou caract√®res).
- Exemple : "Maitre Corbeau sur un arbre perch√©" ‚Üí [Mai ##tre Cor ##beau sur un ar### bre perch√©.].

**Embedding :**
- Associe chaque token √† un vecteur de haute dimension dans un espace continu.
- Exemple : [Mai ##tre Cor ##beau sur un ar### bre perch√©.]. ‚Üí [[0.12, 0.45, ...], [0.34, 0.67, ...], [0.89, 0.23, ...]].

---

# Pourquoi l'Embedding est Important :
- **Compr√©hension S√©mantique :** Les tokens ayant des significations similaires ont des embeddings plus proches dans l'espace vectoriel.


```mermaid
graph LR
  A["Input Phrase: 'Maitre Corbeau sur un arbre perch√©'"] --> B["Tokenization: [Mai ##tre Cor ##beau sur un ar### bre perch√©.]"]
  B --> C["Embedding: Dense Numerical Vectors"]

  C["Tokenization Output"]
  C --> D["Token: 'Mai'"]
  D --> D1["Vector: [0.12, 0.45, 0.78, ...]"]
  C --> E["Token: '##tre'"]
  E --> E1["Vector: [0.34, 0.67, 0.89, ...]"]
  C --> F["Token: 'Cor'"]
  F --> F1["Vector: [0.56, 0.23, 0.91, ...]"]
  C --> G["Token: '##beau'"]
  G --> G1["Vector: [0.78, 0.12, 0.34, ...]"]
  C --> H["Token: 'sur'"]
  H --> H1["Vector: [0.45, 0.89, 0.67, ...]"]
  ```

---



## ü§î Qu‚Äôest-ce que l‚ÄôIA ?
- Intelligence artificielle = capacit√© d‚Äôun programme √† **simuler l‚Äôintelligence humaine**
- Exemple : reconna√Ætre des images, jouer √† des jeux, comprendre le langage

---

## üåü Pourquoi l‚ÄôIA est importante aujourd‚Äôhui
- Smartphones, assistants vocaux, recommandations
- IA pour la sant√©, l‚Äôindustrie, l‚Äôart et la science

---

## 1950 ‚Äì Alan Turing
- Publie "Computing Machinery and Intelligence"
- Propose le **Test de Turing**
- Question : une machine peut-elle penser ?

---

## 1956 ‚Äì Naissance officielle de l‚ÄôIA
- Conf√©rence de **Dartmouth**
- Objectif : cr√©er des machines capables de penser
- D√©but de l‚ÄôIA symbolique

La naissance du mot "Intelligence Artificielle"
**La Conf√©rence de Dartmouth** s'est tenue en 1956 au Dartmouth College dans le New Hampshire, aux √âtats-Unis. Elle a r√©uni des pionniers comme John McCarthy, Marvin Minsky, Claude Shannon et Allen Newell. C'est lors de cette rencontre historique que le terme "Intelligence Artificielle" a √©t√© invent√© par John McCarthy. Les participants pensaient pouvoir cr√©er une machine pensante en quelques mois
https://lh6.googleusercontent.com/2fOknOCOKRB53elLxNJQfA9CGVh1uud99HhsWp2eMJIvCge-mEPiJuKtQN0GIXOPaACYj-OBNccNrBHAzApkaMESTnylTGDqMVciQOM1C10dAXdg1kzKlDIM3jDpFWdz44PWxCJ8

---

## 1960 ‚Äì Perceptrons
- **Frank Rosenblatt** invente le perceptron
- Neurone artificiel = base des r√©seaux de neurones
- Limit√© : ne r√©sout pas les probl√®mes non lin√©aires comme le XOR

---

## üîç Le Probl√®me XOR : Limite du Perceptron Simple

Le perceptron simple ne peut pas r√©soudre le probl√®me **XOR (OU exclusif)**, qui n√©cessite une s√©paration non lin√©aire. Le XOR renvoie vrai uniquement si **une seule** des deux entr√©es est vraie, pas les deux en m√™me temps.

**Pourquoi c'est important ?**
- Cette limitation a montr√© qu'un seul neurone ne suffit pas
- A conduit au **premier hiver de l'IA** (1974-1980) : baisse de financements et d'int√©r√™t pour la recherche


---

## 1980 ‚Äì Perceptrons multicouches
- Introduction des **couches multiples**
- Permet de r√©soudre des probl√®mes plus complexes
- Base des IA modernes

## üß† R√©seaux multicouches : une vraie r√©volution
- Les couches multiples permettent d'apprendre des relations complexes que le perceptron simple ne pouvait pas r√©soudre

---


## üèÜ Les Parrains de l'IA

<div style="display: flex; align-items: center; gap: 20px;">
    <div style="flex: 1;">

**Les P√®res Fondateurs du Deep Learning**

Trois chercheurs ont r√©volutionn√© l'IA moderne et partag√© le **Prix Turing 2018** (le "Nobel de l'informatique") :

**Geoffrey Hinton** üá¨üáß
- R√©seaux de neurones profonds
- R√©tropropagation moderne
- "Parrain du Deep Learning"

**Yann LeCun** üá´üá∑
- R√©seaux convolutifs (CNN)
- Reconnaissance d'images

**Yoshua Bengio** üá®üá¶
- Traitement du langage naturel
- Repr√©sentations distribu√©es
- √âthique de l'IA

https://www.intelligenthq.com/wp-content/uploads/2023/09/godfathers-of-ai.jpg

    </div>

    <div style="flex: 1;">

**Leur Impact**

- Ont persist√© quand personne ne croyait aux r√©seaux de neurones
- Leurs travaux ont permis : reconnaissance vocale, voitures autonomes, traduction automatique
- Forment aujourd'hui la nouvelle g√©n√©ration de chercheurs


    </div>
</div>


---

## 1980s ‚Äì Geoffrey Hinton
- Travaux sur **l‚Äôapprentissage profond**
- Red√©couvre et perfectionne les r√©seaux multicouches
- Pr√©curseur du deep learning moderne

---

## 1980s ‚Äì Yann LeCun
- Travaux sur les **CNN (Convolutional Neural Networks)**
- Applications : reconnaissance de chiffres manuscrits
- D√©but du succ√®s du deep learning

---

## 1990s ‚Äì Yoshua Bengio
- Travaux sur **les repr√©sentations distribu√©es**
- R√©seaux neuronaux plus profonds
- Pr√©curseur des r√©seaux tr√®s larges et profonds actuels

---

## 1997 ‚Äì Deep Blue
- IA d‚ÄôIBM bat Garry Kasparov aux √©checs
- Exemple d‚ÄôIA sp√©cialis√©e
- D√©monstration de puissance de calcul + strat√©gie
![Garry Kasparov vs Deep Blue](https://tse1.mm.bing.net/th/id/OIP.3liapdpAF6vYvBQnLSOGvQHaFA?cb=defcache2defcache=1&rs=1&pid=ImgDetMain&o=7&rm=3)
---

## 2006 ‚Äì Renaissance du Deep Learning
- Hinton et Bengio relancent le deep learning
- Techniques modernes : **r√©seaux profonds et GPU**
- Pr√©paration pour r√©volution visuelle et textuelle

---

## 2012 ‚Äì AlexNet
- R√©seau de neurones convolutif profond
- Gagne le concours **ImageNet**
- R√©volutionne la vision par ordinateur

![Architecture AlexNet](https://i.ytimg.com/vi/ZUc0Mib5DeI/maxresdefault.jpg)

---

## üèÜ AlexNet : La R√©volution de 2012

**Qu'est-ce qu'AlexNet ?**
- R√©seau de neurones convolutif profond cr√©√© par Alex Krizhevsky, Ilya Sutskever et Geoffrey Hinton
- 8 couches (5 convolutives + 3 enti√®rement connect√©es)
- 60 millions de param√®tres

**La Performance**
- Gagne le concours ImageNet 2012
- Taux d'erreur : 15,3% (vs 26,2% pour le second)
- R√©volutionne la reconnaissance d'images

**Les Innovations Cl√©s**
- Utilisation de **GPU** pour l'entra√Ænement

![Architecture AlexNet](https://i.ytimg.com/vi/ZUc0Mib5DeI/maxresdefault.jpg)

---

**Pourquoi c'est important ?**
- Prouve que le deep learning fonctionne
- Lance l'√®re moderne de l'IA
- Inspire tous les mod√®les actuels

    </div>
</div>

---

## üéØ AlexNet : L'√âquipe qui a Chang√© l'IA

**Les Cr√©ateurs**
- **Alex Krizhevsky** : Doctorant, d√©veloppeur principal
- **Ilya Sutskever** : Co-auteur, futur cofondateur d'OpenAI
- **Geoffrey Hinton** : Superviseur, "Parrain du Deep Learning"

---

**Le Destin d'Ilya Sutskever**
- Apr√®s AlexNet, rejoint Google Brain
- 2015 : Cofonde **OpenAI** avec Sam Altman
- R√¥le cl√© dans le d√©veloppement de **GPT** et **ChatGPT**
- Chief Scientist chez OpenAI jusqu'en 2024

---

## üîç Impact d‚ÄôAlexNet
- Montre que le deep learning fonctionne √† grande √©chelle
- GPU rend l‚Äôentra√Ænement possible
- D√©but de la domination du deep learning dans l‚Äôindustrie

---

## üéÆ IA et Jeux vid√©o
- IA apprend en jouant
- Exemple : OpenAI Five, AlphaGo
- Strat√©gie, anticipation, coordination

---

## 2016 ‚Äì AlphaGo
- D√©velopp√© par **Demis Hassabis, DeepMind**
- Bat le champion de Go
- Apprentissage par renforcement + r√©seaux profonds

---

## üéØ Comment AlphaGo a Battu Lee Sedol

**Le Match Historique (Mars 2016)**
- AlphaGo affronte Lee Sedol, champion du monde de Go
- Victoire 4-1 : choc pour la communaut√© du Go
- Consid√©r√© comme impossible 10 ans auparavant

---

**Les Techniques d'AlphaGo**
- **Apprentissage supervis√©** : √©tude de 30 millions de positions de parties de professionnels
- **Apprentissage par renforcement** : l'IA joue contre elle-m√™me des millions de fois
- **Recherche arborescente Monte Carlo** : √©value les meilleurs coups possibles
- Combinaison de r√©seaux neuronaux profonds et d'algorithmes de recherche

---

**Le Coup 37 : Le Moment L√©gendaire**
- Deuxi√®me partie : AlphaGo joue un coup jamais vu auparavant
- Les commentateurs le jugent d'abord "ridicule"
- Se r√©v√®le √™tre un coup de g√©nie qui change la partie
- D√©montre que l'IA peut cr√©er des strat√©gies innovantes

---

**Impact**
- R√©volutionne la compr√©hension du jeu de Go
- Prouve que l'IA peut surpasser l'intuition humaine
- Lee Sedol d√©clare : "AlphaGo m'a montr√© que je ne savais rien"

---

## üåå SETI @ Home
- Projet pour d√©tecter vie extraterrestre
- Utilise la puissance de calcul **des ordinateurs des b√©n√©voles**
- Exemple de **distributed computing** et science collaborative

---

## üéÆ AlphaStar : Champion de StarCraft II

**Le D√©fi StarCraft II**
- Jeu de strat√©gie en temps r√©el extr√™mement complexe
- N√©cessite planification, gestion de ressources, micro-gestion
- Plus de 10^26 actions possibles (vs 10^170 pour le Go)

**Les Performances d'AlphaStar**
- D√©cembre 2018 : Bat des joueurs professionnels
- Atteint le niveau "Grandmaster" (top 0,2% des joueurs)
- G√®re simultan√©ment : √©conomie, arm√©e, strat√©gie

**Les Techniques Utilis√©es**
- **Apprentissage par imitation** : √©tudie des millions de parties humaines
- **Apprentissage par renforcement** : joue contre diff√©rentes versions de lui-m√™me
- **Architecture neuronale** : r√©seaux transformers pour comprendre le contexte du jeu
- Traite environ 22 000 observations par seconde

**Innovation Cl√©**
- AlphaStar ne joue pas de mani√®re surhumaine (APM limit√© √† un niveau humain)
- D√©montre une compr√©hension strat√©gique profonde
- Capable d'adapter sa strat√©gie en temps r√©el

**Impact**
- Prouve que l'IA peut ma√Ætriser des environnements complexes et impr√©visibles
- Applications : logistique, gestion de ressources, planification

---
<!-- .slide: data-layout="text-image" data-img="https://image1.slideserve.com/2915781/brain-size-in-mammals-l.jpg" data-alt="Taille du cerveau chez les mammif√®res" -->

## üß† Le cerveau humain
- Taille moyenne : 1600 cm¬≥
- N√©andertal : 1300 cm¬≥
- Limit√© pour nouveaux neurones
- Synapses : pratiquement illimit√©es

https://image1.slideserve.com/2915781/brain-size-in-mammals-l.jpg
---

## üß¨ AlphaFold : De SETI@Home au Pliage de Prot√©ines

**Le Lien avec SETI@Home**
- SETI@Home : calcul distribu√© pour chercher des signaux extraterrestres
- M√™me principe appliqu√© au pliage de prot√©ines : **Folding@home**
- Des milliers d'ordinateurs volontaires calculent comment les prot√©ines se replient

**AlphaFold : La R√©volution**
- D√©velopp√© par DeepMind (2020)
- R√©sout un probl√®me vieux de 50 ans : pr√©dire la structure 3D des prot√©ines
- Une prot√©ine = cha√Æne d'acides amin√©s qui se replie d'une fa√ßon pr√©cise

**Pourquoi c'est Important ?**
- La forme d'une prot√©ine d√©termine sa fonction
- Comprendre le pliage = comprendre les maladies
- Applications : conception de m√©dicaments, lutte contre les virus

**Les Performances**
- Pr√©dit la structure de 200 millions de prot√©ines
- Pr√©cision comparable aux m√©thodes exp√©rimentales
- R√©duit de plusieurs ann√©es √† quelques heures le temps de recherche

**Impact sur la Science**
- Prix Nobel de Chimie 2024 d√©cern√© √† Demis Hassabis (DeepMind)
- Acc√©l√®re la recherche m√©dicale mondiale
- Donn√©es ouvertes : accessibles √† tous les chercheurs

![AlphaFold Protein Structure](https://cdn.the-scientist.com/assets/articleNo/68887/aImg/43733/alphafold-l.png)

---

## ‚ö° √ânergie : cerveau vs IA
| Syst√®me | Consommation |
|---------|--------------|
| Cerveau humain | ~20 W |
| GPU IA | ~250‚Äì400 W par unit√© |

---

## üß† Comparatif cerveau vs IA
- Cerveau : flexible, √©conome en √©nergie, g√©n√©raliste
- IA : rapide, sp√©cialis√©e, √©nergivore

---

## ‚è≥ Singularit√© technologique
- Moment hypoth√©tique o√π IA surpassera l‚Äôintelligence humaine
- D√©bat : emploi, √©thique, contr√¥le
- Sujet fascinant et controvers√©

---

## ü§ñ L‚ÄôAGI : qu‚Äôest-ce que c‚Äôest ?
- AGI = **Artificial General Intelligence**
- IA capable de comprendre, apprendre et agir **comme un humain**
- Contrairement √† l‚ÄôIA actuelle, qui est sp√©cialis√©e

---

## üß† Diff√©rence IA sp√©cialis√©e vs AGI
| IA sp√©cialis√©e | AGI |
|----------------|-----|
| Fait une seule t√¢che | Peut apprendre toutes les t√¢ches |
| Exemple : AlphaGo | Exemple : r√©soudre un probl√®me, cr√©er, planifier |
| Limit√© √† un domaine | Flexible et g√©n√©raliste |

---

## üåå Pourquoi l‚ÄôAGI est fascinante
- Potentiel √©norme : science, m√©decine, exploration spatiale
- Risques : contr√¥le, √©thique, emploi
- Question cl√© : que se passe-t-il si elle devient **plus intelligente que nous** ?

---

## üîÆ Vers l‚ÄôAGI
- Combinaison :
  - R√©seaux profonds
  - M√©moire et planification
  - Compr√©hension du langage et raisonnement
- Objectif : IA **polyvalente et autonome**

---

## ‚öñÔ∏è √âthique et AGI
- S√©curit√© : √©viter comportements impr√©visibles
- Transparence : comment prend-elle ses d√©cisions ?
- Responsabilit√© : qui contr√¥le l‚ÄôAGI ?

---

## üñ•Ô∏è D√©monstration : Moshi de Kuytai
- G√©n√©ration de texte et images
- Interaction avec le public
- Illustrer puissance et limites de l‚ÄôIA

---

## üßÆ Types d‚Äôapprentissage
- Supervis√© : donn√©es √©tiquet√©es
- Non supervis√© : motifs d√©tect√©s automatiquement
- Par renforcement : essais et erreurs + r√©compenses

---

## üîç Apprentissage supervis√©
- Exemple : reconnaissance d‚Äôimages (chat vs chien)
- IA apprend √† partir d‚Äôexemples connus

---

## üîç Apprentissage non supervis√©
- IA d√©couvre des motifs sans √©tiquettes
- Exemple : clustering, segmentation

---

## üîÑ Apprentissage par renforcement
- IA agit dans un environnement, re√ßoit feedback
- Exemple : AlphaGo, OpenAI Five

---

## ‚öôÔ∏è GPU et Deep Learning
- Calcul parall√®le massif
- Permet l‚Äôentra√Ænement rapide de r√©seaux profonds
- Diff√©rence cl√© avec CPU : vitesse et √©chelle

---

## üåê IA et vie quotidienne
- Smartphones, assistants vocaux
- Recommandations : Netflix, YouTube, Spotify
- Voitures autonomes

---

## üè• IA et sant√©
- D√©tection pr√©coce de maladies
- Analyse d‚Äôimages m√©dicales
- Personnalisation des traitements

---

## üé® IA et cr√©ativit√©
- G√©n√©ration d‚Äôimages, textes, musique
- Limites : cr√©ativit√© encadr√©e, pas d‚Äôintuition


---

# Neurones Artificiels  
<div style="display: flex; align-items: center; gap: 20px;">
    <div style="flex: 1;">

Mod√®le math√©matique du neurone artificiel  

Fonctions d'activation : ReLU, Sigmoid, Tanh  

Similarit√©s et diff√©rences avec les neurones biologiques ?  

</div>

<div style="display: flex; align-items: center; gap: 20px;">
    <div style="flex: 1;">

    ![Structure du Neurone Artificiel](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/Artificial_neuron_structure.svg/1024px-Artificial_neuron_structure.svg.png)  
    *Illustration d'un neurone artificiel*
    </div>
</div>


---

# R√©seaux de Neurones Artificiels

Les r√©seaux de neurones artificiels (RNA) sont des mod√®les computationnels inspir√©s de la structure et du fonctionnement des r√©seaux neuronaux biologiques. 

Ils sont compos√©s de couches interconnect√©es de neurones artificiels, o√π chaque neurone traite les entr√©es, applique une fonction d'activation et transmet la sortie √† la couche suivante. 

**Les RNA** sont largement utilis√©s pour des t√¢ches telles que la reconnaissance de motifs, la classification et la r√©gression dans divers domaines.

---

# Param√®tres et Poids dans les R√©seaux de Neurones

Dans les r√©seaux de neurones, les **param√®tres** font r√©f√©rence aux valeurs ajustables que le mod√®le apprend pendant l'entra√Ænement. Ceux-ci incluent :

**Poids :**

- Repr√©sentent la force de la connexion entre les neurones.
- Ajust√©s pendant l'entra√Ænement pour minimiser l'erreur entre les sorties pr√©dites et r√©elles.

**Biais :**
- Ajout√©s √† la somme pond√©r√©e des entr√©es pour d√©caler la fonction d'activation.
- Aident le mod√®le √† mieux s'adapter aux donn√©es en permettant une flexibilit√© dans les fronti√®res de d√©cision.

---

# Param√®tres et Poids dans les R√©seaux de Neurones

<div style="display: flex; align-items: center; gap: 20px;">
    <div style="flex: 1;">

**Pourquoi Ils Sont Importants :**

- **Les poids et les biais** sont les composants essentiels qui permettent aux r√©seaux de neurones d'apprendre des motifs et de faire des pr√©dictions. En mettant √† jour de mani√®re it√©rative ces valeurs √† l'aide d'algorithmes d'optimisation comme la descente de gradient, le r√©seau am√©liore ses performances sur la t√¢che donn√©e.
    </div>

    <div style="flex: 1;">
**Exemple :**

- Dans un r√©seau de neurones simple, si l'entr√©e est `X`, le poids est `W` et le biais est `B`, la sortie d'un neurone est calcul√©e comme :
$$\text{sortie} = \text{fonction\_activation}(W \cdot X + B)$$

    </div>
</div>
---

# Mistral 7B : Nombre de Param√®tres

<div style="display: flex; align-items: center; gap: 20px;">

    <div style="flex: 1;">

Le mod√®le Mistral 7B est un mod√®le de fondation de pointe avec **7 milliards de param√®tres**.


**Comparaison :**
- **GPT-4 :** Environ 175 milliards de param√®tres estim√©s.
- **LLaMA 2 (13B) :** 13 milliards de param√®tres.

    </div>

    <div style="flex: 1;">

![Comparaison des Param√®tres de Mod√®les](https://www.geeky-gadgets.com/wp-content/uploads/2023/09/New-Mistral-7B-instruct-model-from-Mistral-AI.webp)

    </div>
</div>
---

# Perceptron Multicouche (PMC)
<div style="display: flex; align-items: center; gap: 20px;">

    <div style="flex: 1;">

Un Perceptron Multicouche (PMC) est un type de r√©seau de neurones artificiels compos√© d'une couche d'entr√©e, d'une ou plusieurs couches cach√©es et d'une couche de sortie. Chaque couche consiste en des n≈ìuds (neurones) interconnect√©s o√π les entr√©es sont trait√©es √† travers des connexions pond√©r√©es, des fonctions d'activation et des biais. 

Les PMC sont largement utilis√©s pour des t√¢ches d'apprentissage supervis√© telles que la classification et la r√©gression, tirant parti de leur capacit√© √† mod√©liser des relations complexes et non lin√©aires dans les donn√©es.

Le concept du PMC a √©t√© introduit pour la premi√®re fois en 1969 par Marvin Minsky et Seymour Papert dans leur livre *Perceptrons*, qui a pos√© les bases de la recherche sur les r√©seaux de neurones.

    </div>

    <div style="flex: 1;">

![PMC](https://media.licdn.com/dms/image/D5612AQG2n-h9rBE2NA/article-cover_image-shrink_600_2000/0/1701597139460?e=2147483647&v=beta&t=kTHU5V1z66QpFeikBYqQ4Gwgu-o3V8DlwKWOub6Rr2M)

    </div>

</div>
---



---
# ML : Apprentissage par Renforcement‚Äã

<div style="display: flex; align-items: center; gap: 20px;">
    <div style="flex: 1;">
Agents apprenant par essais et erreurs‚Äã

Syst√®mes de r√©compense‚Äã
    </div>

    <div style="flex: 1;">

<iframe width="560" height="315" src="https://www.youtube.com/embed/spfpBrBjntg?si=68Z-oEMzvfxk8p6x&autoplay=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

    </div>

</div>
---

# ML : Apprentissage par Renforcement‚Äã
<div style="display: flex; align-items: center; gap: 20px;">
    <div style="flex: 1;">

- Syst√®mes de conduite autonome apprenant des strat√©gies de conduite optimales par simulation.
- Syst√®mes de r√©gulateur de vitesse adaptatif optimisant l'efficacit√© √©nerg√©tique et la s√©curit√©.
- Syst√®mes d'assistance au stationnement apprenant √† naviguer dans des sc√©narios de stationnement complexes.

    </div>
    <div style="flex: 1;">  

<iframe width="560" height="315" src="https://www.youtube.com/embed/KPLYhRBCcvk?autoplay=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

**Source :** [AlphaStar : Niveau grand ma√Ætre dans StarCraft II utilisant l'apprentissage par renforcement multi-agents](https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/)

    </div>
</div>
---


## R√©seau de Neurones en Action

<div style="display: flex; align-items: center; gap: 20px;">

    <div style="flex: 1;">

L'animation montre le processus de reconnaissance de chiffres manuscrits √† l'aide d'un r√©seau de neurones. 

Elle visualise comment le mod√®le traite les images d'entr√©e, extrait les caract√©ristiques et pr√©dit le chiffre correspondant.

    </div>

    <div style="flex: 1;">

![R√©seau de Neurones en Action](https://i.makeagif.com/media/3-22-2022/boUeR6.gif)

    </div>
</div>

---

# Base de Donn√©es MNIST : Reconnaissance de Chiffres Manuscrits

<div style="display: flex; align-items: center; gap: 20px;">

    <div style="flex: 1;">

La base de donn√©es MNIST (Modified National Institute of Standards and Technology) est un benchmark largement utilis√© en apprentissage automatique et en vision par ordinateur. Elle se compose de 70 000 images en niveaux de gris de chiffres manuscrits (0 √† 9), chacune de taille 28x28 pixels. La base de donn√©es est utilis√©e pour entra√Æner et √©valuer des mod√®les pour des t√¢ches de reconnaissance de chiffres.

**Importance :**
- MNIST sert de point de d√©part pour tester et comparer les algorithmes d'apprentissage automatique.
- Elle aide √† comprendre comment les r√©seaux de neurones peuvent classifier des nombres bas√©s sur des motifs de pixels.

**Historique :**
- MNIST a √©t√© introduite en 1998 par **Yann LeCun, Corinna Cortes et Christopher J.C. Burges** dans le cadre de leurs recherches sur les r√©seaux de neurones et l'apprentissage automatique.

**Applications :**
- Reconnaissance de chiffres dans les syst√®mes postaux.
- Exp√©riences fondamentales en apprentissage profond.

    </div>
    <div style="flex: 1;">


![Exemple de la Base MNIST](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)

Exemple de reconnaissance de chiffres manuscrits de la base de donn√©es MNIST

    </div>
</div>



---

## **2. Les Bases de l'IA (20 min)**
**Objectif** : Rendre compr√©hensible le fonctionnement de l'IA
- **2.1** Comparaison neurone biologique/neurone artificiel (5 min)
  - Sch√©ma simple : Corps cellulaire/dendrites vs entr√©es/poids
  - Analogie : "Comme une recette de cuisine tr√®s pr√©cise"
- **2.2** Historique accessible (5 min)
  - Alan Turing, Yann Le Cun, SETI@Home
  - Les jeux vid√©o et l'IA (Black & White, Theme Park)
- **2.3** Limites et r√©alit√©s (10 min)
  - Cerveau humain (1600 cm¬≥) vs GPU (consommation √©nerg√©tique)
## üß™ Le Test de Turing : Peut-on Tromper un Humain ?

---
## Le test de Turing
**Principe :**
- Test propos√© par Alan Turing en 1950
- Une machine peut-elle convaincre un humain qu'elle est elle-m√™me humaine lors d'une conversation ?

**Comment √ßa marche :**
1. Un humain dialogue avec deux interlocuteurs cach√©s
2. L'un est une machine, l'autre un humain
3. Si l'humain ne peut pas distinguer qui est qui, la machine "r√©ussit" le test

**Exemple concret :**
- Vous discutez par √©crit avec deux personnes
- L'une parle de ses vacances, l'autre aussi
- Laquelle est l'IA ? Si vous ne pouvez pas le dire, l'IA a r√©ussi !

**Limites du test :**
- R√©ussir ne signifie pas "penser" vraiment
- Une IA peut imiter sans comprendre
- D√©bat : intelligence simul√©e vs intelligence r√©elle

**Aujourd'hui :**
- En 2027 on ne pourra pas faire la disctinction entre un humain et une IA


---

## Applications Concr√®tes (25 min)**
  - G√©n√©ration d'une image simple ("un chat dans un jardin")
  - G√©n√©ration d'un po√®me avec le public
-  Exemples du quotidien (10 min)
  - Reconnaissance vocale, recommandations, traduction automatique
-  D√©tecter l'IA (5 min)
  - "Cette image a-t-elle √©t√© g√©n√©r√©e par une IA ?" (quiz visuel)

---

## **4. Questions Cl√©s (20 min)**
**Objectif** : R√©pondre aux interrogations courantes
- **4.1** "L'IA est-elle plus intelligente que nous ?"
- **4.2** "Peut-on lui faire confiance ?" 
- **4.3** "Va-t-elle nous remplacer ?"