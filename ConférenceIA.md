# <div style="display: flex; justify-content: center; align-items: center; height: 100vh; text-align: center; font-size: 10vw; font-weight: bold; width: 100%;">L'Intelligence Artificielle</div>

---
<!-- .slide: data-layout="two-column" data-img="https://makeaicontent.com/wp-content/uploads/2023/11/DALL%C2%B7E-2023-11-15-19.27.49-A-realistic-image-depicting-harmony-between-human-and-AI-showing-a-human-hand-reaching-out-towards-a-robotic-hand.-The-image-should-capture-a-moment--1024x585.png" -->
## ü§î Qu‚Äôest-ce que l‚ÄôIA ?
- Intelligence artificielle = capacit√© d‚Äôun programme √† **simuler l‚Äôintelligence humaine**

---
<!-- .slide: data-layout="two-column" data-img="https://synoptekmark.b-cdn.net/wp-content/uploads/2023/07/ai-ml-dl-and-generative-ai-face-off.webp" -->
## IA vs IA g√©n√©rative

**Intelligence artificielle (IA)** Imiter l‚Äôintelligence humaine.

**Apprentissage automatique (ML)** Syst√®mes qui apprennent et s‚Äôam√©liorent √† partir de l‚Äôexp√©rience sans √™tre explicitement programm√©s.

**Apprentissage profond (DL)** R√©seaux de neurones pour mod√©liser des motifs complexes dans les donn√©es.

**IA g√©n√©rative** peut cr√©er ou g√©n√©rer de nouveaux contenus, id√©es ou donn√©es qui ressemblent √† la cr√©ativit√© humaine.

---

# <div style="display: flex; justify-content: center; align-items: center; height: 100vh; text-align: center; font-size: 10vw; font-weight: bold; width: 100%;">Histoire de l‚ÄôIA</div>

---
<!-- .slide: data-layout="two-column" data-img="https://cdn.britannica.com/14/84514-050-AF43A0D9/Alan-M-Turing-1951.jpg" -->
## 1950 ‚Äì Alan Turing
- Publie **Computing Machinery and Intelligence**
- Propose le **Test de Turing**
- Question : une machine peut-elle penser ?

---
<!-- .slide: data-layout="two-column" data-img="https://image3.slideserve.com/6546540/turing-s-imitation-game-l.jpg" data-alt="Le Jeu de l'imitation de Turing" -->
## La proposition de Turing (Jeu de l‚Äôimitation)
- Exp√©rience pens√©e o√π un interrogateur converse uniquement par √©crit avec deux entit√©s cach√©es: un humain et une machine.
- Objectif: d√©cider qui est l‚Äôhumain √† partir des r√©ponses en langage naturel.
- R√®gle de succ√®s: si l‚Äôinterrogateur ne distingue pas de fa√ßon fiable la machine de l‚Äôhumain, la machine *r√©ussit* le test.

---
<!-- .slide: data-layout="two-column" data-img="https://cryptoconexion.com/wp-content/uploads/2023/08/Dartmouth-1956-Tw.jpg" -->

## 1956 ‚Äì Naissance officielle de l‚ÄôIntelligence Artificielle
- Conf√©rence de **Dartmouth**
- Objectif : cr√©er des machines capables de penser

> Le terme **Intelligence Artificielle** est invent√©

---

## La Conf√©rence de Dartmouth
En 1956 au Dartmouth College dans le New Hampshire, aux √âtats-Unis. 
 
Elle a r√©uni **John McCarthy, Marvin Minsky, Claude Shannon et Allen Newell**. 

C'est lors de cette rencontre historique que le terme **Intelligence Artificielle** a √©t√© invent√© par John McCarthy. 

> Les participants pensaient pouvoir cr√©er une machine pensante en quelques mois ...

---
<!-- .slide: data-layout="two-column" data-img="https://perceptrondemo.com/assets/rosenblatt-wiring-perceptron-940c6e47.jpg" -->
## 1960 ‚Äì Le Perceptron
- **Frank Rosenblatt** invente le perceptron
- Neurone artificiel = base des r√©seaux de neurones
- Limit√© : ne r√©sout pas les probl√®mes non lin√©aires comme le XOR

---
<!-- .slide: data-layout="two-column" data-img="https://miro.medium.com/max/1290/1*LSEtAtqzAtIP8A7G4gdDMA@2x.jpeg" -->
## Le perceptron de Rosenblatt en bref
- Neurone binaire: somme pond√©r√©e des entr√©es + biais, puis seuil.
- R√©sout les probl√®mes lin√©airement s√©parables (AND, OR).
- Limite majeure: XOR non s√©parable ‚Üí besoin de couches cach√©es.

---
<!-- .slide: data-layout="two-column" data-img="https://news.cornell.edu/sites/default/files/styles/story_thumbnail_xlarge/public/2019-09/0925_rosenblatt_main.jpg?itok=BCWmlVvO" -->

## L'intuition de Rosenblatt

> "Devices of this sort are expected ultimately to be capable of concept formation, language translation, collation of military intelligence, and the solution of problems through inductive logic."
<div style="font-size: 0.6em; line-height: 1.3;">
<strong>Traduction :</strong><br>
¬´ On s‚Äôattend √† ce que des dispositifs de ce type soient, √† terme, capables de la formation de concepts, de la traduction de langues, de la compilation de renseignements militaires et de la r√©solution de probl√®mes par la logique inductive. ¬ª
</div>

<em>Frank Rosenblatt, 1957</em>

---
<!-- .slide: data-layout="two-column" data-img="https://media.geeksforgeeks.org/wp-content/uploads/20240328125139/XOR-Gate.png" -->
## üîç Le Probl√®me XOR : Limite du Perceptron Simple

Le perceptron simple ne peut pas r√©soudre le probl√®me **XOR (OU exclusif)**, qui n√©cessite une s√©paration non lin√©aire.

**Pourquoi c'est important ?**
> Cette limitation a conduit au **premier hiver de l'IA** (1974-1980) : baisse de financements et d'int√©r√™t pour la recherche

---
<!-- .slide: data-layout="two-column" data-img="https://media.licdn.com/dms/image/D5612AQG2n-h9rBE2NA/article-cover_image-shrink_600_2000/0/1701597139460?e=2147483647&v=beta&t=kTHU5V1z66QpFeikBYqQ4Gwgu-o3V8DlwKWOub6Rr2M" -->
## 1980 ‚Äì Le Perceptron multicouche
- Introduction des **couches multiples**
- Permet de r√©soudre des probl√®mes plus complexes
- Base des IA modernes

---

## <div style="display: flex; justify-content: center; align-items: center; height: 100vh; text-align: center; font-size: 8vw; font-weight: bold; width: 100%;">Au fait c'est quoi un Neurone Artificiel, et c'est quoi un R√©seau de Neurones Artificiels ?</div>

---

## Un Neurone Biologique
<!-- .slide: data-layout="two-column" data-img="https://www.researchgate.net/profile/Christos-Pliatsikas/publication/376253955/figure/fig1/AS:11431281218483806@1705590629078/Neuron-anatomy-Created-with-BioRendercom.png" -->

**Structure :**  
- Dendrites  
- Soma  
- Axone  

**Fonctionnement des synapses :**  
- Transmission de signaux chimiques et √©lectriques  

---
<!-- .slide: data-layout="two-column" data-img="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/Artificial_neuron_structure.svg/1024px-Artificial_neuron_structure.svg.png" -->
## Un Neurone Artificiel

Mod√®le math√©matique du neurone artificiel

Fonctions d‚Äôactivation : ReLU, Sigmo√Øde, Tanh

Similarit√©s et diff√©rences avec les neurones biologiques ?

---
<!-- .slide: data-layout="two-column" data-video="https://datascientest.com/wp-content/uploads/2020/06/DP_2.png" -->

## R√©seaux de Neurones Artificiels

Les r√©seaux de neurones artificiels sont des mod√®les computationnels inspir√©s de la structure et du fonctionnement des r√©seaux neuronaux biologiques. 

Ils sont compos√©s de couches interconnect√©es de neurones artificiels, o√π chaque neurone traite les entr√©es, applique une fonction d'activation et transmet la sortie √† la couche suivante. 

> Les R√©seaux de Neurones Artificiels sont les unit√©s fondamentales des IA.

---
<!-- .slide: data-layout="two-column" data-img="https://i.makeagif.com/media/3-22-2022/boUeR6.gif" -->
## R√©seau de Neurones en Action

L'animation montre le processus de reconnaissance de chiffres manuscrits √† l'aide d'un r√©seau de neurones. 

Elle visualise comment le mod√®le traite les images d'entr√©e, extrait les caract√©ristiques et pr√©dit le chiffre correspondant.

---

<!-- .slide: data-layout="two-column" data-video="https://youtu.be/FwFduRA_L6Q?si=seVi3mjawRWwdIj1" -->

## D√©monstration de r√©seau convolutionnel de 1989 par Yann Le Cun

**LeNet-1** premier r√©seau convolutionnel capable de reconna√Ætre des chiffres manuscrits avec une bonne vitesse et pr√©cision.

Il a √©t√© d√©velopp√©e d√©but 1989 au d√©partement **Adaptive System Research**, dirig√© par **Larry Jackel**, chez Bell Labs √† Holmdel (New Jersey).

Cette d√©monstration ¬´ en temps r√©el ¬ª tournait sur une carte DSP install√©e dans un PC 486, avec une cam√©ra vid√©o et une carte d‚Äôacquisition. 

---
<!-- .slide: data-layout="two-column" data-img="https://www.intelligenthq.com/wp-content/uploads/2023/09/godfathers-of-ai.jpg" data-alt="Les P√®res Fondateurs du Deep Learning" -->

## üèÜ Les Parrains de l'IA

**Les P√®res Fondateurs du Deep Learning**

Trois chercheurs ont r√©volutionn√© l'IA moderne et partag√© le **Prix Turing 2018** (le "Nobel de l'informatique") :

- Ont persist√© quand personne ne croyait aux r√©seaux de neurones
- Leurs travaux ont permis : reconnaissance vocale, voitures autonomes, traduction automatique

**Geoffrey Hinton** üá¨üáß

**Yann Le Cun** üá´üá∑

**Yoshua Bengio** üá®üá¶

---
<!-- side.slide: data-layout="two-column" data-img="https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/Geoffrey_E._Hinton%2C_2024_Nobel_Prize_Laureate_in_Physics_%283x4_cropped%29.jpg/250px-Geoffrey_E._Hinton%2C_2024_Nobel_Prize_Laureate_in_Physics_%283x4_cropped%29.jpg" -->


## 1980s ‚Äì Geoffrey Hinton
- Travaux sur **l‚Äôapprentissage profond**
- Red√©couvre et perfectionne les r√©seaux multicouches
- Pr√©curseur du deep learning moderne


---
<!-- .slide: data-layout="two-column" data-img="https://i.la-croix.com/x/smart/2016/03/01/1200743436/Yann-LeCun-responsable-laboratoire-intelligence-artificielle-chez-Facebook_0.jpg"-->
## 1980s ‚Äì Yann LeCun
- Travaux sur les **CNN (Convolutional Neural Networks)**
- Applications : reconnaissance de chiffres manuscrits
- D√©but du succ√®s du deep learning

---
<!-- .slide: data-layout="two-column" data-img="https://www.actuia.com/storage/uploads/2018/04/yoshua-bengio.jpg" data-alt="Yoshua Bengio" -->
## 1990s ‚Äì Yoshua Bengio
- Travaux sur **les repr√©sentations distribu√©es**
- R√©seaux neuronaux plus profonds
- Pr√©curseur des r√©seaux tr√®s larges et profonds actuels

---
<!-- .slide: data-layout="two-column" data-img="https://tse1.mm.bing.net/th/id/OIP.3liapdpAF6vYvBQnLSOGvQHaFA" -->

## 1997 ‚Äì Deep Blue
- L'ordinateur d‚ÄôIBM bat Garry Kasparov aux √©checs
- D√©monstration de force brute d‚Äôalgorithmes

---
<!-- .slide: data-layout="two-column" data-img="https://miro.medium.com/v2/resize:fit:640/format:webp/1*tnFwtQfQUsPsmFmOlJtUSw.png" -->
## ImageNet (en bref)

- Jeu de donn√©es d‚Äôimages √† grande √©chelle lanc√© en 2009
- ~14 M d‚Äôimages annot√©es √† la main, ~20 000 cat√©gories (synsets WordNet)
- Utilis√© pour entra√Æner et √©valuer des mod√®les de vision par ordinateur

---

<!-- .slide: data-layout="two-column" data-img="https://i.ytimg.com/vi/ZUc0Mib5DeI/maxresdefault.jpg" -->

## üèÜ AlexNet : La R√©volution de 2012

- R√©seau de neurones convolutif profond cr√©√© par **Alex Krizhevsky, Ilya Sutskever et Geoffrey Hinton**
- 8 couches (5 convolutives + 3 enti√®rement connect√©es)
- 60 millions de param√®tres

---
<!-- .slide: data-layout="two-column" data-img="https://www.pinecone.io/_next/image/?url=https%3A%2F%2Fcdn.sanity.io%2Fimages%2Fvr8gru94%2Fproduction%2F1937562f4ac2507386e0a1965602544f697bb439-665x419.png&w=750&q=75" -->
## üèÜ AlexNet : La R√©volution de 2012

**Gagne le concours ImageNet 2012**
- Taux d'erreur : 15,3% (vs 26,2% pour le second)
- R√©volutionne la reconnaissance d'images

---
<!-- .slide: data-layout="two-column" data-img="https://computerhistory.org/wp-content/uploads/2025/01/fig-69-REDUCED-bedroom-computer-used-for-breakthrough-1024x771.jpg" -->

## üèÜ AlexNet : La R√©volution de 2012

**Les Innovations Cl√©s**
- Utilisation de **GPU Nvidia** pour l'entra√Ænement
- **NVIDIA GTX 580 GPU** avec 3GB de m√©moire
- Ordinateur de chambre utilis√© pour la perc√©e

---

<!-- .slide: data-layout="two-column" data-img="https://www.zdnet.com/a/img/resize/cbdfcc9ffe02c07ec17d656be49e670a55e467ec/2025/03/20/1fff3c66-1148-433b-859b-e53ca710522c/u-of-toronto-2013-hinton-krizhevsky-sutskever.jpg?auto=webp&width=1280" data-alt="Geoffrey Hinton, Alex Krizhevsky et Ilya Sutskever (Universit√© de Toronto, 2013)" -->

## üéØ AlexNet : L'√âquipe qui a Chang√© l'IA

**Les Cr√©ateurs**
- **Alex Krizhevsky** : Doctorant, d√©veloppeur principal
- **Ilya Sutskever** : Co-auteur, futur cofondateur d'OpenAI
- **Geoffrey Hinton** : Superviseur, "Parrain du Deep Learning"

---
<!-- .slide: data-layout="two-column" data-img="https://d92co48ro6fll.cloudfront.net/gradual/videos/scale/transformX/posters/what-s-next-for-ai-systems-language-models-with-ilya-sutskever-of-openai.jpg" data-alt="Ilya Sutskever" -->

## Le fabuleux destin d'Ilya Sutskever
- Apr√®s AlexNet, rejoint **Google Brain**
- 2015 : Cofonde **OpenAI** avec Sam Altman
- R√¥le cl√© dans le d√©veloppement de **GPT** et **ChatGPT**
- Chief Scientist chez **OpenAI** jusqu'en 2024

---
<!-- .slide: data-layout="two-column" data-img="https://i.huffpost.com/gen/4072998/images/o-JEU-DE-GO-facebook.jpg" -->

## Le d√©fi du jeu de Go
- Jeu de strat√©gie chinois vieux de plus de 2500 ans
- Complexit√© extr√™me: il y a plus de positions possibles que d'atomes dans l'univers observable

> Seul l'intuition humaine peut le ma√Ætriser

---
<!-- .slide: data-layout="two-column" data-video="https://www.youtube.com/watch?v=g-dKXOlsf98" -->
## 2016 - AlphaGo
- D√©velopp√© par **Demis Hassabis, DeepMind**
- Apprentissage par renforcement + r√©seaux profonds

---
<!-- .slide: data-layout="two-column" data-img="https://imgv2-1-f.scribdassets.com/img/document/698305505/original/462b61234a/1707724024?v=1" -->
## AlphaGo bat Lee Sedol 4-1

- 2016 AlphaGo affronte **Lee Sedol**, champion du monde de Go
- **Victoire 4-1** : choc pour la communaut√© du Go
- Consid√©r√© comme impossible 10 ans auparavant


---
<!-- .slide: data-layout="two-column" data-video="https://youtu.be/whNvUWRQPhY" -->

## Le Coup 37 : Le Moment L√©gendaire
- Deuxi√®me partie : AlphaGo joue un coup jamais vu auparavant
- Les commentateurs le jugent d'abord **ridicule**, une erreur grossi√®re
- Se r√©v√®le √™tre un coup de g√©nie qui change la partie

> AlphaGo m'a montr√© que je ne savais rien

<em>Lee Sedol</em>

---
<!-- .slide: data-layout="two-column" data-img="https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExZzZoZXBhZnU1M2Q0ajZ0cmE0cWdwcm83Y25pNmk1NXp2OW4xNnpvYyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/fVK2mkgyIHUkw/giphy.gif" -->

## Le D√©fi StarCraft II
- Jeu de strat√©gie en temps r√©el
- Extr√™mement complexe
- N√©cessite planification, gestion de ressources, micro-gestion
- Brouillard de guerre : informations incompl√®tes

---
<!-- .slide: data-layout="two-column" data-video="https://www.youtube.com/watch?v=cUTMhmVh1qs" -->
## D√©cembre 2018 : AlphaStar bat des joueurs professionnels de StarCraft II
- Atteint le niveau **Grandmaster** (top 0,2% des joueurs)
- **AlphaStar** est devenue une IA pro de **Starcraft 2** en s'entra√Ænant, comme un humain
- **A jou√© plus de 200 ans** de parties en 2 semaines
- **Fais preuve de cr√©ativit√© et d'intuition**

---
## üß¨ Pliage mol√©culaire en m√©decine

- La forme d‚Äôune prot√©ine dicte son r√¥le (ex. enzymes, anticorps, r√©cepteurs).
- Trop de formes possibles (nombre de combinaisons astronomique).
- Le pliage d√©pend de nombreux facteurs (eau, liaisons, ions, pH, temp√©rature, etc.).
- **Demande des capacit√©s de calcul √©normes.**

---
<!-- .slide: data-layout="two-column" data-img="https://iadatavision.wordpress.com/wp-content/uploads/2023/03/alphafold-db-1.png" -->

## AlphaFold : La R√©volution
- D√©velopp√© par DeepMind (2020)
- R√©sout un probl√®me vieux de 50 ans : pr√©dire la structure 3D des prot√©ines
- Pr√©dit la structure de 200 millions de prot√©ines (soit quasiment toutes les prot√©ines du vivant)
---
<!-- .slide: data-layout="two-column" data-video="https://youtu.be/gg7WjuFs8F4?si=k0zLPdsV-yJ4RBKs" -->

## AlphaFold
- A fait avancer la recherche biom√©dicale de plusieurs d√©cades en quelques mois
- **Avant AlphaFold**, les scientifiques avaient d√©termin√© exp√©rimentalement environ **200 000 structures de prot√©ines**
- **Avec AlphaFold**, plus de **200 millions de structures de prot√©ines** qui sont d√©sormais disponibles gratuitement
- **Prix Nobel de Chimie 2024** d√©cern√© conjointement √† Demis Hassabis (DeepMind), John Jumper (DeepMind) et David Baker (University of Washington)

---

<!-- .slide: data-layout="two-column" data-img="https://cbmm.mit.edu/sites/default/files/styles/colorbox_for_node_images/public/news-events/65e80469-36b7-44de-bb8c-0ddb5b00.jpeg?itok=br6JZi3g" -->

## Qui est Demis Hassabis ?
- N√© en 1976 √† Londres, Royaume-Uni.
- Enfance : prodige des √©checs, atteint le rang de ma√Ætre international √† 13 ans.
- √âtudes : dipl√¥m√© en informatique de l‚ÄôUniversit√© de Cambridge, doctorat en neurosciences cognitives √† UCL.

---

## Demis Hassabis et les Jeux vid√©o üéÆ
- A commenc√© sa carri√®re dans les jeux vid√©o √† 17 ans.
- **Syndicate (1993)**
R√¥le : Playtester (testeur de jeu) dans ses d√©buts chez Bullfrog Productions.

- **Theme Park (1994)**
Lead Programmer (programmeur principal)

- **Republic: The Revolution (2003)**
R√¥le : Executive Designer (concepteur ex√©cutif) et concept principal du jeu.
Studio : Elixir Studios, soci√©t√© qu‚Äôil a fond√©e en 1998 √† Londres.

- **Evil Genius (2004)**
R√¥le : Executive Designer et Concept.
Studio : Elixir Studios, toujours sous sa direction.

---

<!-- .slide: data-layout="two-column" data-img="https://news.aikoreacommunity.com/content/images/2024/01/20240109_181100.png" -->
## 2017 ‚Äì Attention is All You Need
- le Transformer est une architecture d‚ÄôIA (2017) qui comprend le contexte des phrases gr√¢ce √† un m√©canisme appel√© **attention**.
- **l‚Äôattention permet au mod√®le de se concentrer sur les mots les plus utiles de la phrase**

---
<!-- .slide: data-layout="two-column" data-img="https://inside-machinelearning.com/wp-content/uploads/2021/10/AttentionViz.png" -->

## Le M√©canisme de l‚ÄôAttention
- Petite illustration: dans la phrase **Le chat de ma grand m√®re mange la souris**, 
- L‚Äôattention se concentre sur **mange** et **souris**, pas sur **chat** ou **grand m√®re**.
- Avant la machine aurait compris que **ma grand m√®re qui mange la souris !**

---

## √âquation du m√©canisme d‚Äôattention

Le m√©canisme d‚Äôattention peut s‚Äôexprimer math√©matiquement ainsi :

$$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V$$

O√π :
- $Q$ (Query) : ce que l‚Äôon recherche
- $K$ (Key) : ce avec quoi on compare
- $V$ (Value) : l‚Äôinformation √† r√©cup√©rer
- $d_k$ : dimension des vecteurs de cl√©s (utilis√©e pour le facteur d‚Äô√©chelle)

---
<!-- .slide: data-layout="two-column" data-img="https://miro.medium.com/v2/resize:fit:1200/1*QhE-IttZzUBITxMsK74QSQ.png" -->
## üéØ Impact des Transformers
- R√©volutionne le traitement du langage naturel (NLP)
- Base des mod√®les comme BERT, GPT, T5

> C'est le T dans GPT !

---
<!-- .slide: data-layout="two-column" data-img="https://www.tooltester.com/wp-content/uploads/2023/02/ChatGPT-launch-timeline-to-GPT-4-768x439.jpg" -->

## üöÄ ChatGPT !
- Adoption record (>100 M utilisateurs/mois)
- Popularisation de l‚ÄôIA g√©n√©rative et acc√©l√©ration de son adoption dans tous les secteurs

---

## Quiz: Que signifie GPT ?

- A) General Principles of Technology
- B) Guided Pretrained Text
- C) Generative Pretrained Transformer

---

## A quoi sert le M√©canisme d‚ÄôAttention ?
- A) √Ä am√©liorer la vitesse de calcul
- B) √Ä permettre au mod√®le de se concentrer sur les parties importantes du texte
- C) √Ä √™tre concentr√© plus longtemps pendant les conf√©rences 
---

# <div style="display: flex; justify-content: center; align-items: center; height: 100vh; text-align: center; font-size: 10vw; font-weight: bold; width: 100%;">Comment la machine apprend ?</div>

---
<!-- .slide: data-layout="two-column" data-img="https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2020/07/Supervised-Learning-in-ML.jpg" -->
## ML : Apprentissage supervis√©

Utilisation de donn√©es √©tiquet√©es

T√¢ches de classification et de r√©gression

---
<!-- .slide: data-layout="two-column" data-img="https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2020/07/Unsupervised-Learning-in-ML.jpg" -->

## ML : Apprentissage non supervis√©

D√©couverte de structures cach√©es

Techniques de clustering et de r√©duction de dimensionnalit√©

---
<!-- .slide: data-layout="two-column" data-video="https://www.youtube.com/embed/spfpBrBjntg?si=68Z-oEMzvfxk8p6x&autoplay=1&mute=1" data-mute="true" -->

## ML : Apprentissage par Renforcement‚Äã

Agents apprenant par essais et erreurs‚Äã

Syst√®mes de r√©compense‚Äã

---
<!-- .slide: data-layout="two-column" data-video="https://www.youtube.com/embed/KPLYhRBCcvk?autoplay=1&mute=1" data-mute="true" -->

## ML : Apprentissage par Renforcement‚Äã

- Syst√®mes de conduite autonome apprenant des strat√©gies de conduite optimales par simulation.
- Syst√®mes de r√©gulateur de vitesse adaptatif optimisant l'efficacit√© √©nerg√©tique et la s√©curit√©.
- Syst√®mes d'assistance au stationnement apprenant √† naviguer dans des sc√©narios de stationnement complexes.

**Source :** [AlphaStar : Niveau grand ma√Ætre dans StarCraft II utilisant l'apprentissage par renforcement multi-agents](https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/)

---

## <div style="display: flex; justify-content: center; align-items: center; height: 100vh; text-align: center; font-size: 10vw; font-weight: bold; width: 100%;">Maitre Corbeau sur un arbre _____ </div>

---

## Ma√Ætre Corbeau sur un arbre <u>perch√©</u>
![Poesie](http://andreetgyps.a.n.pic.centerblog.net/o/6b0e0247.jpg)

---

## "Ma√Ætre Corbeau sur un arbre perch√©" ‚Üí d√©casyllabe
- **Syllabes dans un po√®me :** Ma√Æ-tre Cor-beau sur un ar-bre per-ch√©.
- **Tokens en NLP :** [Ma√Æ ##tre Cor ##beau sur un ar ##bre per ##ch√©.].

---
<!-- .slide: data-layout="two-column" data-img="https://cdn.prod.website-files.com/61e7d259b7746e3f63f0b6be/6630e466c569a5f73cd81c9e_Understanding%20LLM%20Billing_%20From%20Characters%20to%20Tokens-p-800.jpg"-->
## Les Tokens sont aux LLMs ce que les syllabes sont √† la po√©sie

Tout comme les syllabes sont les √©l√©ments constitutifs du rythme et de la structure d'un po√®me, les tokens sont les unit√©s fondamentales qui permettent aux mod√®les d'IA de traiter et de comprendre le texte.

---

## Tokens dans les Mod√®les d'IA

La limite de tokens d√©finit le nombre maximum de tokens qu'un mod√®le peut traiter dans une seule entr√©e. Des limites de tokens plus √©lev√©es permettent de g√©rer des contextes plus longs, rendant les mod√®les plus efficaces pour des t√¢ches comme la synth√®se, l'analyse de code et la g√©n√©ration de documents.

| Mod√®le         | Taille Max (tokens) | Pages de Livre de Poche |
|----------------|---------------------|------------------------------|
| GPT-5          | 128 000             | ~512                         |
| Llama 3.1      | 128 000             | ~512                         |
| Mistral Large  | 64 000              | ~256                         |

---
<!-- .slide: data-layout="two-column" data-img="https://causewriter.ai/wp-content/uploads/2023/08/image-2.png)" -->

## Embedding: Transformer les Tokens en Repr√©sentations Num√©riques

L'**embedding** transforme les **tokens** en **vecteurs**, qui servent de points d'entr√©e pour le **LLM**.

- Associe chaque token √† un vecteur de haute dimension dans un espace continu.
- Exemple : 
- [Mai ##tre Cor ##beau sur un ar### bre perch√©.]. ‚Üí [[0.12, 0.45, ...], [0.34, 0.67, ...], [0.89, 0.23, ...]].

---

## Param√®tres et Poids dans les R√©seaux de Neurones

- **Les poids et les biais** sont les param√®tres  d'un r√©seaux de neurone. En mettant √† jour de mani√®re it√©rative ces valeurs √† l'aide d'algorithmes d'optimisation comme **la descente de gradient**, le r√©seau am√©liore ses performances sur la t√¢che donn√©e.

**Poids et biais au niveau math√©matique :**

- Dans un r√©seau de neurones simple, si l'entr√©e est `X`, le poids est `W` et le biais est `B`, et $f$ est la fonction d'activation.

La sortie d'un neurone est calcul√©e comme :

$$\text{sortie} = f(W \cdot X + B)$$


---
<!-- .slide: data-layout="two-column" data-img="https://shearwaterjapan.com/wp-content/uploads/2025/04/llm-300x200.png" -->
## un LLMs est un mod√®le avec beaucoup beaucoup ... beaucoup de param√®tres
- **L**: Large
- **L**: Language
- **M**: Model

---
<!-- .slide: data-layout="two-column" data-img="https://www.geeky-gadgets.com/wp-content/uploads/2023/09/New-Mistral-7B-instruct-model-from-Mistral-AI.webp" -->
## Mistral 7B : Nombre de Param√®tres

Le mod√®le Mistral 7B est un mod√®le de fondation avec **7 milliards de param√®tres**.

**Comparaison :**
- **GPT-4 :** Environ 175 milliards de param√®tres estim√©s.
- **LLaMA 2 (13B) :** 13 milliards de param√®tres.
- **GPT-5 :** entre 500 et 1500 milliards de param√®tres selon les rumeurs.

---
<!-- .slide: data-layout="two-column" data-video="https://www.youtube.com/embed/FdZvMoP0dRU?autoplay=1&mute=1" data-mute="true" -->
## YOLO

**Y**ou

**O**nly

**L**ook

**O**nce

- Mod√®le de d√©tection d'objets en temps r√©el
---

## Comment la voiture voit-elle le monde ?

<div style="display: flex; justify-content: center; align-items: center; height: 100vh; width: 100%;">
    <video controls autoplay muted playsinline width="640" height="480">
        <source src="https://digitalassets.tesla.com/tesla-contents/video/upload/f_auto,q_auto/network.mp4" type="video/mp4">
        Votre navigateur ne prend pas en charge la balise vid√©o.
    </video>
</div>


---
<!-- .slide: data-layout="two-column" data-img="https://media.licdn.com/dms/image/v2/D5612AQEjGGrWMf79pA/article-cover_image-shrink_720_1280/B56ZYz6vWpGoAM-/0/1744627806105?e=2147483647&v=beta&t=Bd-5S7UUOmFMZ_8UUHvqIosaJ5EohNQRUyn-nbO62E0" -->
## Les Successeurs des LLM (vision de Yann Le Cun)
- JEPA : apprentissage auto-supervis√© qui pr√©dit des parties manquantes.
- World Models : mod√®les pr√©dictifs du monde (vid√©o/audio/action) pour raisonner et planifier.

---

# <div style="display: flex; justify-content: center; align-items: center; height: 100vh; text-align: center; font-size: 10vw; font-weight: bold; width: 100%;">Comment l'Homme se positionne vis √† vis de l'IA ?</div>

---

<!-- .slide: data-layout="text-image" data-img="https://image1.slideserve.com/2915781/brain-size-in-mammals-l.jpg" data-alt="Taille du cerveau chez les mammif√®res" -->

## üß† Le cerveau humain
- Taille moyenne : 1600 cm¬≥
- N√©andertal : 1800 cm¬≥

|                      | Cerveau humain | Intelligence Artificielle |
|----------------------|----------------|----------------|
| Consommation         | ~20 W          | ~1 000 MW      |
| Neurones (approx.)   | ~86 milliards  | ~300 millions (GPT-5)  |
| Nouveaux neurones    | Limit√© par la boite cranienne | No limit|

Le cerveau humain est aujourd'hui **100 millions de fois plus √©conome en √©nergie** qu'une IA.

---

## Comparaison des performances de l'IA et des humains
![Comparaison des performances de l'IA et des humains ‚Äî Our World in Data](https://upload.wikimedia.org/wikipedia/commons/1/11/Comparaison_des_performances_de_l%27IA_et_des_humains_-_Our_World_in_Data.svg?download)

---
# <div style="display: flex; justify-content: center; align-items: center; height: 100vh; text-align: center; font-size: 10vw; font-weight: bold; width: 100%;">Et Demain ?</div>

---
changement de paradigme
1er revolution indus : lnenergie se transforme en force
2eme energie devient con'aissance (Internet)
3   energie devient intelligence (QI)

---
<!-- .slide: data-layout="two-column" data-img="https://www.geeky-gadgets.com/wp-content/uploads/2023/06/What-is-AGI.jpg" -->
## L'AGI: Artificial General Intelligence

L'IA capable de comprendre, apprendre et agir **comme un humain**

| IA sp√©cialis√©e | AGI |
|----------------|-----|
| Fait une seule t√¢che | Peut apprendre toutes les t√¢ches |
| Exemple : AlphaGo | Exemple : r√©soudre un probl√®me, cr√©er, planifier |
| Limit√© √† un domaine | **polyvalente et autonome** |

---
<!-- .slide: data-layout="two-column" data-img="https://media.licdn.com/dms/image/D4D12AQGyGyJI7Ht9fw/article-cover_image-shrink_600_2000/0/1668837711973?e=2147483647&v=beta&t=aGrDI4vzxLQz976zymt5s0DWTCp6GeG6UXLtKzmOxns" -->

## La Singularit√© Technologique
- Point hypoth√©tique o√π l‚ÄôIA d√©passe l‚Äôintelligence humaine et s‚Äôauto-am√©liore rapidement.
- Timeline incertaine
- Sc√©nario graduel ou abrupt ?

---

## √† votre avis la singularit√© technologique arrivera quand ?
- jamais
- dans 50 ans
- dans 20 ans

---
<!-- .slide: data-layout="two-column" data-img="https://miro.medium.com/v2/resize:fit:1358/1*na6eVIVet02RemFEjSDA4w.png" -->
## Loi de Moore appliqu√©e √† l'IA
- Chaque nouvelle g√©n√©ration de mod√®les d‚ÄôIA multiplie par 10 sa capacit√© √† interval fixe

---

<!-- .slide: data-layout="two-column" data-img="https://content.api.news/v3/images/bin/e39c59e786ff255e6a7d1c4ae9d9611b" data-alt="Terence Tao" -->
## Terence Tao
- M√©daille Fields 2006
- QI estim√© (non officiel) : ~220, L'homme le plus intelligent du monde
- Th√©or√®me Green‚ÄìTao: progressions arithm√©tiques dans les nombres premiers

> En 2030, une IA sera t'elle capable de r√©soudre la plupart des probl√®mes math√©matiques que Terence Tao peut r√©soudre ?

---

<!-- .slide: data-layout="two-column" data-img="https://controverity.com/wp-content/uploads/2026/01/elon2026-1024x536.webp" -->

## La singularity arrive avec 30 ans d'avance

> 2026 est l'ann√©e de la singularit√© technologique.

Elon Musk le 4 janvier 2026

---

<!-- .slide: data-layout="two-column" data-video="https://www.youtube.com/watch?v=zGfac0-MY20" data-mute="false" -->

## L'IA est elle meilleure que nous ?
Ecoutons ce qu'en pense C√©dric Villani 
- Math√©maticien fran√ßais, *m√©daille Fields 2010* pour ses travaux en th√©orie cin√©tique (√©quations de Boltzmann et de Landau).
- Auteur du rapport national 2018 sur l‚ÄôIA *Donner un sens √† l‚Äôintelligence artificielle* (strat√©gie fran√ßaise et europ√©enne).
- Ancien directeur de l‚ÄôInstitut Henri-Poincar√©

---

# <div style="display: flex; justify-content: center; align-items: center; height: 100vh; text-align: center; font-size: 10vw; font-weight: bold; width: 100%;">Un futur pas si lointain ...</div>

---

<!-- .slide: data-layout="two-column" data-img="https://www.numerama.com/wp-content/uploads/2023/04/concoursia-cover.jpg" -->

## L‚Äôimage d‚Äôune IA a dup√© les organisateurs du plus prestigieux concours de photos
Le 14 avril 2023, l‚Äôartiste allemand Boris Eldagsen a gagn√© la cat√©gorie ¬´ Open ¬ª du **Sony World Photography Awards**, 

Probl√®me : la photo pr√©sent√©e a √©t√© r√©alis√©e en partie avec de l‚Äôintelligence artificielle.

---
## IA et humains : vers une indistinction ?
<!-- .slide: data-layout="two-column" data-video="https://youtu.be/ARxHvTScXMY?si=K77qc-_mvKUfFskK" -->
D√®s 2027 on ne pourra pas faire la distinction entre un humain et une IA

**Prompt** *A stylish woman walks down a Tokyo street filled with warm glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots, and carries a black purse. She wears sunglasses and red lipstick. She walks confidently and casually. The street is damp and reflective, creating a mirror effect of the colorful lights. Many pedestrians walk about.*

---
<!-- .slide: data-layout="two-column" data-img="https://www.eoi.es/blogs/redinnovacionEOI/files/2015/08/Jan2V_Film_Her.jpg" -->
## 2013 üé¨ Film "Her" avec Joaquin Phoenix
- Relation intime entre un homme et une IA avanc√©e
- Th√®mes: attachement √©motionnel, empathie simul√©e, solitude

---
<!-- .slide: data-layout="two-column" data-img="https://static.milibris.com/thumbnail/issue/1a2ac231-e35e-41be-a8f2-e3bf978e79a7/front/catalog-cover-large.jpeg" -->

## Magazine Lib√©ration du 22 janvier 2026

Le test de Turing est officiellement pass√© par toute IA grand public qui tient aujourd'hui dans votre poche.

---

<!-- .slide: data-layout="two-column" data-img="https://img-api.mac4ever.com/1179/0/af7e438e7e_qu-est-ce-que-le-slop-ce-curieux-phenomene-envahissant.webp" -->

## IA Slop ou la bouillie g√©n√©r√©e par IA qui pollue le web

- Contenu g√©n√©r√© automatiquement de faible qualit√©, produit √† grande √©chelle.

> "It‚Äôs becoming harder to detect what‚Äôs real and what‚Äôs AI-generated. This is particularly critical when it comes to deepfakes. 

Neal Mohan, CEO de YouTube, mercredi 21 janvier 2026.

---
<!-- .slide: data-layout="two-column" data-img="https://www.editions-harmattan.fr/media/359309/download/9782336526898r.jpg?v=1" -->
## la  course √† l'IA entretenue par les tensions g√©opolitiques entre les blocs USA CHINE EUROPE

L‚ÄôIA devient un champ de bataille entre les √âtats-Unis, la Chine et l‚ÄôEurope

---

## Atelier Interactif : Comment d√©tecter une image g√©n√©r√©e par IA ?

https://this-person-does-not-exist.com/en

---

## Parlons avec le G√©n√©ral de Gaulle
https://unmute.sh/

---
# <div style="display: flex; justify-content: center; align-items: center; height: 100vh; text-align: center; font-size: 10vw; font-weight: bold; width: 100%;">Merci pour votre attention</div>

---

# <div style="display: flex; justify-content: center; align-items: center; height: 100vh; text-align: center; font-size: 10vw; font-weight: bold; width: 100%;">Questions ?</div>

---

# <div style="display: flex; justify-content: center; align-items: center; height: 100vh; text-align: center; font-size: 10vw; font-weight: bold; width: 100%;">Pour aller plus loin ...</div>

---

## Base de Donn√©es MNIST : Reconnaissance de Chiffres Manuscrits


La base de donn√©es MNIST (Modified National Institute of Standards and Technology) est un benchmark largement utilis√© en apprentissage automatique et en vision par ordinateur. Elle se compose de 70 000 images en niveaux de gris de chiffres manuscrits (0 √† 9), chacune de taille 28x28 pixels. La base de donn√©es est utilis√©e pour entra√Æner et √©valuer des mod√®les pour des t√¢ches de reconnaissance de chiffres.

**Importance :**
- MNIST sert de point de d√©part pour tester et comparer les algorithmes d'apprentissage automatique.
- Elle aide √† comprendre comment les r√©seaux de neurones peuvent classifier des nombres bas√©s sur des motifs de pixels.

**Historique :**
- MNIST a √©t√© introduite en 1998 par **Yann LeCun, Corinna Cortes et Christopher J.C. Burges** dans le cadre de leurs recherches sur les r√©seaux de neurones et l'apprentissage automatique.

**Applications :**
- Reconnaissance de chiffres dans les syst√®mes postaux.
- Exp√©riences fondamentales en apprentissage profond.


![Exemple de la Base MNIST](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)

Exemple de reconnaissance de chiffres manuscrits de la base de donn√©es MNIST

---
<!-- .slide: data-layout="two-column" data-img="https://media.licdn.com/dms/image/D5612AQG2n-h9rBE2NA/article-cover_image-shrink_600_2000/0/1701597139460?e=2147483647&v=beta&t=kTHU5V1z66QpFeikBYqQ4Gwgu-o3V8DlwKWOub6Rr2M" -->

## Le Perceptron Multicouche

Le **Perceptron Multicouche** est un type de r√©seau de neurones artificiels compos√© d'une couche d'entr√©e, d'une ou plusieurs couches cach√©es et d'une couche de sortie. 

Chaque couche consiste en des neurones interconnect√©s o√π les entr√©es sont trait√©es √† travers des connexions pond√©r√©es, des fonctions d'activation et des biais.

Le concept du PMC a √©t√© introduit pour la premi√®re fois en 1969 par *Marvin Minsky et Seymour Papert* dans leur livre *Perceptrons*

---

## Le test de Turing
**Principe :**
- Test propos√© par Alan Turing en 1950
- Une machine peut-elle convaincre un humain qu'elle est elle-m√™me humaine lors d'une conversation ?

---

<!-- .slide: data-layout="two-column" data-img="https://media.geeksforgeeks.org/wp-content/uploads/Turing-Diagram-159676.png" -->

## Comment √ßa marche :
1. Un humain dialogue avec deux interlocuteurs cach√©s
2. L'un est une machine, l'autre un humain
3. Si l'humain ne peut pas distinguer qui est qui, la machine "r√©ussit" le test

---

<!-- .slide: data-layout="two-column" data-img="https://medias.pourlascience.fr/api/v1/images/view/5a82ac588fe56f7c1c01b7e3/wide_1000-webp/image.jpg" -->
## Les Techniques d'AlphaGo
- **Apprentissage supervis√©** : √©tude de 30 millions de positions de parties de professionnels
- **Apprentissage par renforcement** : l'IA joue contre elle-m√™me des millions de fois

---

<!-- .slide: data-layout="two-column" data-video="https://youtu.be/UuhECwm31dM?si=5-9yNHVsPns0mCSq" -->

## L'IA dans StarCraft II bat le meilleur joueur humain

> La grande incertitude [li√©e au manque] d'informations en p√©riode de guerre est d'une difficult√© particuli√®re parce que toutes les actions doivent dans une certaine mesure √™tre planifi√©es avec une l√©g√®re zone d'ombre (‚Ä¶) comme l'effet d'un brouillard [...].

‚Äî Carl von Clausewitz, *De la guerre*

---
<!-- .slide: data-layout="two-column" data-video="https://youtu.be/EyWsnc7cB_w?si=BvUJi0RrmLqog1BR" -->

## 1999 SETI@Home
- Calcul distribu√© pour chercher des signaux extraterrestres
- Des milliers d'ordinateurs volontaires cherchent des signaux extraterrestres dans les donn√©es radio

---
<!-- .slide: data-layout="two-column" data-video="https://www.youtube.com/watch?v=b8GS9l3xZiY" -->
## Foldit 
- Un jeu o√π des joueurs s'amusent √† plier des prot√©ines,  bas√© sur le moteur Rosetta (University of Washington).
- Exploite **l‚Äôintuition humaine** pour explorer l‚Äôespace de conformations, g√©n√©rant des donn√©es utiles √† l‚ÄôIA et √† la conception de m√©dicaments.

---

<!-- .slide: data-layout="two-column" data-img="https://techcrunch.com/wp-content/uploads/2019/01/motionalpha.gif" -->
## Les Techniques Utilis√©es
- **Apprentissage par imitation** : √©tudie des millions de parties humaines
- **Apprentissage par renforcement** : joue contre diff√©rentes versions de lui-m√™me
- **A jou√© plus de 200 ans** de parties en 2 semaines
- **Fais preuve de cr√©ativit√© et d'intuition**

---
## Questions Cl√©s
- "L'IA est-elle plus intelligente que nous ?"
- "Peut-on lui faire confiance ?" 
- "Va-t-elle nous remplacer ?"