## <div style="display: flex; justify-content: center; align-items: center; height: 100vh; text-align: center; font-size: 10vw; font-weight: bold; width: 100%;">L'Intelligence Artificielle</div>


---

## ü§î Qu‚Äôest-ce que l‚ÄôIA ?
- Intelligence artificielle = capacit√© d‚Äôun programme √† **simuler l‚Äôintelligence humaine**
- Exemple : reconna√Ætre des images, jouer √† des jeux, comprendre le langage

---

## üåü Pourquoi l‚ÄôIA est importante aujourd‚Äôhui
- Smartphones, assistants vocaux, recommandations
- IA pour la sant√©, l‚Äôindustrie, l‚Äôart et la science


---
<!-- .slide: data-layout="two-column" data-img="https://synoptekmark.b-cdn.net/wp-content/uploads/2023/07/ai-ml-dl-and-generative-ai-face-off.webp" -->
## AI VS GENERATIVE AI‚Äã

<div style="display: flex; align-items: center; gap: 20px;">
  <div style="flex: 1;">

**ARTIFICIAL INTELLIGENCE** is a field of computer science that aims to create systems capable of imitating or simulating human intelligence.‚Äã

**MACHINE LEARNING** focuses on building systems that learn and improve from experience without being explicitly programmed.‚Äã

**DEEP LEARNING** uses neural networks with many layers to model complex patterns in data.‚Äã

**GENERATIVE AI** can create or generate new content, ideas, or data that resemble human creativity.‚Äã

---

# <div style="display: flex; justify-content: center; align-items: center; height: 100vh; text-align: center; font-size: 10vw; font-weight: bold; width: 100%;">Histoire de l‚ÄôIA</div>

---

## 1950 ‚Äì Alan Turing
- Publie "Computing Machinery and Intelligence"
- Propose le **Test de Turing**
- Question : une machine peut-elle penser ?

---
<!-- .slide: data-layout="two-column" data-img="https://image3.slideserve.com/6546540/turing-s-imitation-game-l.jpg" data-alt="Le Jeu de l'imitation de Turing" -->
## La proposition de Turing (Jeu de l‚Äôimitation)
- Exp√©rience pens√©e o√π un interrogateur converse uniquement par √©crit avec deux entit√©s cach√©es: un humain et une machine.
- Objectif: d√©cider qui est l‚Äôhumain √† partir des r√©ponses en langage naturel.
- R√®gle de succ√®s: si l‚Äôinterrogateur ne distingue pas de fa√ßon fiable la machine de l‚Äôhumain, la machine *r√©ussit* le test.
- Motivation: remplacer ‚ÄúLes machines peuvent-elles penser ?‚Äù par une √©valuation comportementale mesurable.
- Contraintes: √©change textuel (sans vision ni audio), sujets libres, dur√©e limit√©e.


---
<!-- .slide: data-layout="two-column" data-img="https://lh6.googleusercontent.com/2fOknOCOKRB53elLxNJQfA9CGVh1uud99HhsWp2eMJIvCge-mEPiJuKtQN0GIXOPaACYj-OBNccNrBHAzApkaMESTnylTGDqMVciQOM1C10dAXdg1kzKlDIM3jDpFWdz44PWxCJ8" -->

## 1956 ‚Äì Naissance officielle de l‚ÄôIA
- Conf√©rence de **Dartmouth**
- Objectif : cr√©er des machines capables de penser

---

## La Conf√©rence de Dartmouth
En 1956 au Dartmouth College dans le New Hampshire, aux √âtats-Unis. 
 
Elle a r√©uni **John McCarthy, Marvin Minsky, Claude Shannon et Allen Newell**. 

C'est lors de cette rencontre historique que le terme **Intelligence Artificielle** a √©t√© invent√© par John McCarthy. 

Les participants pensaient pouvoir cr√©er une machine pensante en quelques mois ...

---

## 1960 ‚Äì Le Perceptron
- **Frank Rosenblatt** invente le perceptron
- Neurone artificiel = base des r√©seaux de neurones
- Limit√© : ne r√©sout pas les probl√®mes non lin√©aires comme le XOR

---

## Le perceptron de Rosenblatt en bref
- Neurone binaire: somme pond√©r√©e des entr√©es + biais, puis seuil.
- R√©sout les probl√®mes lin√©airement s√©parables (AND, OR).
- Limite majeure: XOR non s√©parable ‚Üí besoin de couches cach√©es.


---


## Le perceptron de Rosenblatt en bref

> "Devices of this sort are expected ultimately to be capable of concept formation, language translation, collation of military intelligence, and the solution of problems through inductive logic."
 <em>‚Äî Frank Rosenblatt, 1957</em>

---

## üîç Le Probl√®me XOR : Limite du Perceptron Simple

Le perceptron simple ne peut pas r√©soudre le probl√®me **XOR (OU exclusif)**, qui n√©cessite une s√©paration non lin√©aire. Le XOR renvoie vrai uniquement si **une seule** des deux entr√©es est vraie, pas les deux en m√™me temps.

**Pourquoi c'est important ?**
- Cette limitation a conduit au **premier hiver de l'IA** (1974-1980) : baisse de financements et d'int√©r√™t pour la recherche

---

## 1980 ‚Äì Perceptrons multicouches
- Introduction des **couches multiples**
- Permet de r√©soudre des probl√®mes plus complexes
- Base des IA modernes

---

## üß† R√©seaux multicouches : une vraie r√©volution
- Les couches multiples permettent d'apprendre des relations complexes que le perceptron simple ne pouvait pas r√©soudre

---

## Le Perceptron Multicouche (PMC)

Un Perceptron Multicouche (PMC) est un type de r√©seau de neurones artificiels compos√© d'une couche d'entr√©e, d'une ou plusieurs couches cach√©es et d'une couche de sortie. 

Chaque couche consiste en des n≈ìuds (neurones) interconnect√©s o√π les entr√©es sont trait√©es √† travers des connexions pond√©r√©es, des fonctions d'activation et des biais. 

Le concept du PMC a √©t√© introduit pour la premi√®re fois en 1969 par **Marvin Minsky et Seymour Papert** dans leur livre *Perceptrons*, qui a pos√© les bases de la recherche sur les r√©seaux de neurones.

![PMC](https://media.licdn.com/dms/image/D5612AQG2n-h9rBE2NA/article-cover_image-shrink_600_2000/0/1701597139460?e=2147483647&v=beta&t=kTHU5V1z66QpFeikBYqQ4Gwgu-o3V8DlwKWOub6Rr2M)

---

## Au fait c'est quoi un Neurone Artificiel et un R√©seau de Neurones Artificiels ?

---

## Neurones biologiques  
<!-- .slide: data-layout="two-column" data-img="https://www.researchgate.net/profile/Christos-Pliatsikas/publication/376253955/figure/fig1/AS:11431281218483806@1705590629078/Neuron-anatomy-Created-with-BioRendercom.png" -->

**Structure :**  
- Dendrites  
- Soma  
- Axone  

**Fonctionnement des synapses :**  
- Transmission de signaux chimiques et √©lectriques  


---
<!-- .slide: data-layout="two-column" data-img="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/Artificial_neuron_structure.svg/1024px-Artificial_neuron_structure.svg.png" -->
## Un Neurone Artificiel

Mod√®le math√©matique du neurone artificiel  

Fonctions d‚Äôactivation : ReLU, Sigmo√Øde, Tanh  

Similarit√©s et diff√©rences avec les neurones biologiques ?  

---
<!-- .slide: data-layout="two-column" data-video="https://www.youtube.com/embed/spfpBrBjntg?si=68Z-oEMzvfxk8p6x" -->

## R√©seaux de Neurones Artificiels

Les r√©seaux de neurones artificiels sont des mod√®les computationnels inspir√©s de la structure et du fonctionnement des r√©seaux neuronaux biologiques. 

Ils sont compos√©s de couches interconnect√©es de neurones artificiels, o√π chaque neurone traite les entr√©es, applique une fonction d'activation et transmet la sortie √† la couche suivante. 

Les R√©seaux de Neurones Artificiels sont les unit√©s fondamentales des IA.

---

<!-- .slide: data-layout="text-video" data-video="https://youtu.be/FwFduRA_L6Q?si=seVi3mjawRWwdIj1" -->

## D√©monstration de r√©seau convolutionnel de 1989 par Yann Le Cun

*LeNet-1* premier r√©seau convolutionnel capable de reconna√Ætre des chiffres manuscrits avec une bonne vitesse et pr√©cision.

Elle a √©t√© d√©velopp√©e d√©but 1989 au d√©partement Adaptive System Research, dirig√© par Larry Jackel, chez Bell Labs √† Holmdel (New Jersey).

Cette d√©monstration ¬´ en temps r√©el ¬ª tournait sur une carte DSP install√©e dans un PC 486, avec une cam√©ra vid√©o et une carte d‚Äôacquisition. 

---
<!-- .slide: data-layout="two-column" data-img="https://www.intelligenthq.com/wp-content/uploads/2023/09/godfathers-of-ai.jpg" data-alt="Les P√®res Fondateurs du Deep Learning" -->

## üèÜ Les Parrains de l'IA

**Les P√®res Fondateurs du Deep Learning**

Trois chercheurs ont r√©volutionn√© l'IA moderne et partag√© le **Prix Turing 2018** (le "Nobel de l'informatique") :

**Geoffrey Hinton** üá¨üáß

**Yann LeCun** üá´üá∑

**Yoshua Bengio** üá®üá¶

---

**Leur Impact**

- Ont persist√© quand personne ne croyait aux r√©seaux de neurones
- Leurs travaux ont permis : reconnaissance vocale, voitures autonomes, traduction automatique
- Forment aujourd'hui la nouvelle g√©n√©ration de chercheurs

---

## 1980s ‚Äì Geoffrey Hinton
- Travaux sur **l‚Äôapprentissage profond**
- Red√©couvre et perfectionne les r√©seaux multicouches
- Pr√©curseur du deep learning moderne

---

## 1980s ‚Äì Yann LeCun
- Travaux sur les **CNN (Convolutional Neural Networks)**
- Applications : reconnaissance de chiffres manuscrits
- D√©but du succ√®s du deep learning

---

## 1990s ‚Äì Yoshua Bengio
- Travaux sur **les repr√©sentations distribu√©es**
- R√©seaux neuronaux plus profonds
- Pr√©curseur des r√©seaux tr√®s larges et profonds actuels

---
<!-- .slide: data-layout="text-image" data-img="https://tse1.mm.bing.net/th/id/OIP.3liapdpAF6vYvBQnLSOGvQHaFA?cb=defcache2defcache=1&rs=1&pid=ImgDetMain&o=7&rm=3" data-alt="Garry Kasparov vs Deep Blue]" -->

## 1997 ‚Äì Deep Blue
- IA d‚ÄôIBM bat Garry Kasparov aux √©checs
- D√©monstration de force brute d‚Äôalgorithmes

---

## 2006 ‚Äì Renaissance du Deep Learning
- Hinton et Bengio relancent le deep learning
- Techniques modernes : **r√©seaux profonds et GPU**
- Pr√©paration pour r√©volution visuelle et textuelle

---

## 2012 ‚Äì AlexNet
- R√©seau de neurones convolutif profond
- Gagne le concours **ImageNet**
- R√©volutionne la vision par ordinateur

![Architecture AlexNet](https://i.ytimg.com/vi/ZUc0Mib5DeI/maxresdefault.jpg)

---

## ImageNet (en bref)

- Jeu de donn√©es d‚Äôimages √† grande √©chelle lanc√© en 2009
- ~14 M d‚Äôimages annot√©es √† la main, ~20 000 cat√©gories (synsets WordNet)
- Utilis√© pour entra√Æner et √©valuer des mod√®les de vision par ordinateur
![ImageNet](https://cv.gluon.ai/_images/imagenet_banner.jpeg)

---

## üèÜ AlexNet : La R√©volution de 2012

**Qu'est-ce qu'AlexNet ?**
- R√©seau de neurones convolutif profond cr√©√© par Alex Krizhevsky, Ilya Sutskever et Geoffrey Hinton
- 8 couches (5 convolutives + 3 enti√®rement connect√©es)
- 60 millions de param√®tres

![Architecture AlexNet](https://i.ytimg.com/vi/ZUc0Mib5DeI/maxresdefault.jpg)

---

## üèÜ AlexNet : La R√©volution de 2012

**La Performance**
- Gagne le concours ImageNet 2012
- Taux d'erreur : 15,3% (vs 26,2% pour le second)
- R√©volutionne la reconnaissance d'images

---

## üèÜ AlexNet : La R√©volution de 2012

**Les Innovations Cl√©s**
- Utilisation de **GPU Nvidia** pour l'entra√Ænement

![Ordinateur de chambre utilis√© pour la perc√©e](https://computerhistory.org/wp-content/uploads/2025/01/fig-69-REDUCED-bedroom-computer-used-for-breakthrough-1024x771.jpg)

---

**Pourquoi c'est important ?**
- Prouve que le deep learning fonctionne
- Lance l'√®re moderne de l'IA
- Inspire **tous** les mod√®les actuels

---

<!-- .slide: data-layout="two-column" data-img="https://www.zdnet.com/a/img/resize/cbdfcc9ffe02c07ec17d656be49e670a55e467ec/2025/03/20/1fff3c66-1148-433b-859b-e53ca710522c/u-of-toronto-2013-hinton-krizhevsky-sutskever.jpg?auto=webp&width=1280" data-alt="Geoffrey Hinton, Alex Krizhevsky et Ilya Sutskever (Universit√© de Toronto, 2013)" -->

## üéØ AlexNet : L'√âquipe qui a Chang√© l'IA

**Les Cr√©ateurs**
- **Alex Krizhevsky** : Doctorant, d√©veloppeur principal
- **Ilya Sutskever** : Co-auteur, futur cofondateur d'OpenAI
- **Geoffrey Hinton** : Superviseur, "Parrain du Deep Learning"

---
<!-- .slide: data-layout="two-column" data-img="https://d92co48ro6fll.cloudfront.net/gradual/videos/scale/transformX/posters/what-s-next-for-ai-systems-language-models-with-ilya-sutskever-of-openai.jpg" data-alt="Ilya Sutskever" -->

## Le fabuleux destin d'Ilya Sutskever
- Apr√®s AlexNet, rejoint Google Brain
- 2015 : Cofonde **OpenAI** avec Sam Altman
- R√¥le cl√© dans le d√©veloppement de **GPT** et **ChatGPT**
- Chief Scientist chez OpenAI jusqu'en 2024

---

## üîç Impact d‚ÄôAlexNet
- Montre que le deep learning fonctionne √† grande √©chelle
- GPU rend l‚Äôentra√Ænement possible
- D√©but de la domination du deep learning dans l‚Äôindustrie

---

## üéÆ IA et Jeux vid√©o
- IA apprend en jouant
- Exemple : OpenAI Five, AlphaGo
- Strat√©gie, anticipation, coordination

---

## 2016 - AlphaGo
- D√©velopp√© par **Demis Hassabis, DeepMind**
- Bat le champion de Go
- Apprentissage par renforcement + r√©seaux profonds

---
<!-- .slide: data-layout="two-column" data-img="https://imgv2-1-f.scribdassets.com/img/document/698305505/original/462b61234a/1707724024?v=1" -->
## üéØ Comment AlphaGo a battu Lee Sedol

**Le Match Historique (Mars 2016)**
- AlphaGo affronte Lee Sedol, champion du monde de Go
- Victoire 4-1 : choc pour la communaut√© du Go
- Consid√©r√© comme impossible 10 ans auparavant

---

## Les Techniques d'AlphaGo
- **Apprentissage supervis√©** : √©tude de 30 millions de positions de parties de professionnels
- **Apprentissage par renforcement** : l'IA joue contre elle-m√™me des millions de fois
- **Recherche arborescente Monte Carlo** : √©value les meilleurs coups possibles
- Combinaison de r√©seaux neuronaux profonds et d'algorithmes de recherche

---
<!-- .slide: data-layout="two-column" data-video="https://youtu.be/whNvUWRQPhY" -->

## Le Coup 37 : Le Moment L√©gendaire
- Deuxi√®me partie : AlphaGo joue un coup jamais vu auparavant
- Les commentateurs le jugent d'abord "ridicule"
- Se r√©v√®le √™tre un coup de g√©nie qui change la partie
- D√©montre que l'IA peut cr√©er des strat√©gies innovantes

---

## Impact
- R√©volutionne la compr√©hension du jeu de Go
- Prouve que l'IA peut surpasser l'intuition humaine 
> "AlphaGo m'a montr√© que je ne savais rien"
Lee Sedol

---

## üåå SETI @ Home
- Projet pour d√©tecter vie extraterrestre
- Utilise la puissance de calcul **des ordinateurs des b√©n√©voles**
- Exemple de **distributed computing** et science collaborative

---

## üéÆ AlphaStar : Champion de StarCraft II

**Le D√©fi StarCraft II**
- Jeu de strat√©gie en temps r√©el extr√™mement complexe
- N√©cessite planification, gestion de ressources, micro-gestion
- Plus de 10^26 actions possibles (vs 10^170 pour le Go)

---

**Les Performances d'AlphaStar**
- D√©cembre 2018 : Bat des joueurs professionnels
- Atteint le niveau "Grandmaster" (top 0,2% des joueurs)
- G√®re simultan√©ment : √©conomie, arm√©e, strat√©gie

---

**Les Techniques Utilis√©es**
- **Apprentissage par imitation** : √©tudie des millions de parties humaines
- **Apprentissage par renforcement** : joue contre diff√©rentes versions de lui-m√™me
- **Architecture neuronale** : r√©seaux transformers pour comprendre le contexte du jeu
- Traite environ 22 000 observations par seconde

---
**Innovation Cl√©**
- AlphaStar ne joue pas de mani√®re surhumaine (APM limit√© √† un niveau humain)
- D√©montre une compr√©hension strat√©gique profonde
- Capable d'adapter sa strat√©gie en temps r√©el

---
<!-- .slide: data-layout="two-column" data-video="https://youtu.be/UuhECwm31dM?si=5-9yNHVsPns0mCSq" -->
## L'IA dans StarCraft II bat le meilleur joueur humain

> La grande incertitude [li√©e au manque] d'informations en p√©riode de guerre est d'une difficult√© particuli√®re parce que toutes les actions doivent dans une certaine mesure √™tre planifi√©es avec une l√©g√®re zone d'ombre qui (‚Ä¶) comme l'effet d'un brouillard ou d'un clair de lune, donne aux choses des dimensions exag√©r√©es ou non naturelles.

‚Äî Carl von Clausewitz, "De la guerre"

---
<!-- .slide: data-layout="two-column" data-img="https://cdn.mos.cms.futurecdn.net/uMHimeHetVYcCSt8ExUM8.jpg" -->

## SETI : Recherche d‚Äôintelligences extraterrestres
- Objectif : d√©tecter des technosignatures (√©missions radio √©troites, impulsions laser) d‚Äôorigine non naturelle.
- M√©thodes : radiot√©lescopes (Allen Telescope Array, Green Bank), observations optiques, analyse de spectres, filtrage des interf√©rences terrestres.
- Probl√®me : le volume de recherche immense et pas assez de capacit√© de calcul.


---
<!-- .slide: data-layout="two-column" data-video="https://youtu.be/EyWsnc7cB_w?si=BvUJi0RrmLqog1BR" -->

## 1999 SETI@Home
- Calcul distribu√© pour chercher des signaux extraterrestres
- Des milliers d'ordinateurs volontaires cherchent des signaux extraterrestres dans les donn√©es radio

---
## üß¨ Pliage mol√©culaire en m√©decine

## Pourquoi c‚Äôest important
- La forme d‚Äôune prot√©ine dicte son r√¥le (ex. enzymes, anticorps, r√©cepteurs).
- Un mauvais pliage peut provoquer des maladies (Alzheimer, Parkinson, mucoviscidose).
- Aide √† concevoir des m√©dicaments cibl√©s et √† mieux diagnostiquer.

---

## Pourquoi c‚Äôest difficile
- Trop de formes possibles (nombre de combinaisons astronomique).
- Le pliage d√©pend de nombreux facteurs (eau, liaisons, ions, pH, temp√©rature, etc.).
- Les mod√®les informatiques sont co√ªteux et doivent √™tre v√©rifi√©s en laboratoire.

---

## Impact

- Meilleure compr√©hension de la fonction des prot√©ines et moins d‚Äôeffets ind√©sirables.
- Conception plus pr√©cise de th√©rapies.
- D√©couvertes acc√©l√©r√©es gr√¢ce au calcul avanc√© et √† l‚ÄôIA.


---
<!-- .slide: data-layout="two-column" data-img="https://upload.wikimedia.org/wikipedia/en/3/35/LifeWithPlayStation_Folding.jpg" -->

## üß¨ Folding@home : Pliage des Prot√©ines Distribu√©
- Projet de calcul distribu√© lanc√© en 2000 par Stanford pour simuler le repliement des prot√©ines.
- Des volontaires pr√™tent CPU/GPUleur ordinateurs pour ex√©cuter des calculs.
- Objectifs: comprendre le repliement, les dysfonctionnements et interactions, acc√©l√©rer la recherche sur Alzheimer, cancers, maladies infectieuses.

---

**AlphaFold : La R√©volution**
- D√©velopp√© par DeepMind (2020)
- R√©sout un probl√®me vieux de 50 ans : pr√©dire la structure 3D des prot√©ines
- Une prot√©ine = cha√Æne d'acides amin√©s qui se replie d'une fa√ßon pr√©cise

**Pourquoi c'est Important ?**
- La forme d'une prot√©ine d√©termine sa fonction
- Comprendre le pliage = comprendre les maladies
- Applications : conception de m√©dicaments, lutte contre les virus

---

https://youtu.be/gg7WjuFs8F4?si=k0zLPdsV-yJ4RBKs


**Les Performances**
- Pr√©dit la structure de 200 millions de prot√©ines
- Pr√©cision comparable aux m√©thodes exp√©rimentales
- R√©duit de plusieurs ann√©es √† quelques heures le temps de recherche

**Impact sur la Science**
- Prix Nobel de Chimie 2024 d√©cern√© √† Demis Hassabis (DeepMind)
- Acc√©l√®re la recherche m√©dicale mondiale
- Donn√©es ouvertes : accessibles √† tous les chercheurs

![AlphaFold Protein Structure](https://cdn.the-scientist.com/assets/articleNo/68887/aImg/43733/alphafold-l.png)

---

## Demis Hassabis et les Jeux vid√©o üéÆ  

- Syndicate (1993)
	‚Ä¢	R√¥le : Playtester (testeur de jeu) dans ses d√©buts chez Bullfrog Productions.  Ôøº
	‚Ä¢	Studio : Bullfrog Productions (connu pour des jeux de simulation et strat√©gie innovants).  Ôøº

- Theme Park (1994)
	‚Ä¢	Lead Programmer (programmeur principal) sur plusieurs versions (DOS, Amiga, SNES, PlayStation, etc.).  Ôøº
	‚Ä¢	Studio : Bullfrog Productions (√©diteur / d√©veloppeur).  Ôøº
üëâ Theme Park est un jeu de simulation de parc d‚Äôattractions tr√®s populaire dans les ann√©es 1990.

- Republic: The Revolution (2003)
	‚Ä¢	R√¥le : Executive Designer (concepteur ex√©cutif) et concept principal du jeu.  Ôøº
	‚Ä¢	Studio : Elixir Studios, soci√©t√© qu‚Äôil a fond√©e en 1998 √† Londres.  Ôøº
üëâ Un jeu de strat√©gie / simulation politique ambitieux con√ßu autour de syst√®mes complexes.

- Evil Genius (2004)
	‚Ä¢	R√¥le : Executive Designer et Concept.  Ôøº
	‚Ä¢	Studio : Elixir Studios, toujours sous sa direction.  Ôøº
üëâ Dans ce jeu, le joueur incarne un ma√Ætre du mal qui dirige une base secr√®te (similaire au style ‚Äúsimulation de base‚Äù avec humour).


---

<!-- .slide: data-layout="two-column" data-img="https://news.aikoreacommunity.com/content/images/2024/01/20240109_181100.png" -->
## 2017 ‚Äì Attention is All You Need
- En bref: le Transformer est une architecture d‚ÄôIA (2017) qui comprend le contexte des phrases gr√¢ce √† un m√©canisme appel√© ‚Äúattention‚Äù.
- Id√©e cl√©: l‚Äôattention permet au mod√®le de se concentrer sur les mots les plus utiles pour la t√¢che (traduire, r√©sumer, r√©pondre).
- Petite illustration: dans ‚ÄúLe chat de ma grand m√®re mange la souris‚Äù, le mot ‚Äúmange‚Äù regarde ‚Äúchat‚Äù et ‚Äúsouris‚Äù pour comprendre qui fait quoi.

---

## üéØ Impact des Transformers
- R√©volutionne le traitement du langage naturel (NLP)
- Base des mod√®les comme BERT, GPT, T5

> C'est le T dans GPT !

---

## üöÄ OpenAI et l‚Äôimpact de ChatGPT
- Lancement en nov. 2022, adoption record (>100 M utilisateurs/mois)
- Popularisation de l‚ÄôIA g√©n√©rative et acc√©l√©ration de son adoption dans tous les secteurs

---

# <div style="display: flex; justify-content: center; align-items: center; height: 100vh; text-align: center; font-size: 10vw; font-weight: bold; width: 100%;">Comment la machine apprend</div>

---

## üßÆ Types d‚Äôapprentissage
- Supervis√© : donn√©es √©tiquet√©es
- Non supervis√© : motifs d√©tect√©s automatiquement
- Par renforcement : essais et erreurs + r√©compenses

---

## üîç Apprentissage supervis√©
- Exemple : reconnaissance d‚Äôimages (chat vs chien)
- IA apprend √† partir d‚Äôexemples connus

---

## üîç Apprentissage non supervis√©
- IA d√©couvre des motifs sans √©tiquettes
- Exemple : clustering, segmentation

---

## üîÑ Apprentissage par renforcement
- IA agit dans un environnement, re√ßoit feedback
- Exemple : AlphaGo, OpenAI Five

---

## ML : Apprentissage supervis√©

<div style="display: flex; align-items: center; gap: 20px;">
    <div style="flex: 1;">

Utilisation de donn√©es √©tiquet√©es

T√¢ches de classification et de r√©gression

    </div>

    <div style="flex: 1;">

![Apprentissage supervis√©](https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2020/07/Supervised-Learning-in-ML.jpg)

    </div>
</div>
---

## ML : Apprentissage supervis√©

- Maintenance pr√©dictive des composants du v√©hicule (ex. plaquettes de frein, pneus).
- Analyse du comportement des conducteurs et √©valuation du risque.
- Reconnaissance et classification des panneaux de signalisation.
- Syst√®mes d‚Äôalerte de franchissement de ligne.

---

## ML : Apprentissage non supervis√©

<div style="display: flex; align-items: center; gap: 20px;">
    <div style="flex: 1;">

D√©couverte de structures cach√©es

Techniques de clustering et de r√©duction de dimensionnalit√©
    </div>

    <div style="flex: 1;">

![Apprentissage non supervis√©](https://techvidvan.com/tutorials/wp-content/uploads/sites/2/2020/07/Unsupervised-Learning-in-ML.jpg)

    </div>
    
</div>

---

## ML : Apprentissage non supervis√©
- Regroupement des profils de conduite pour des offres d‚Äôassurance personnalis√©es.
- Regroupement des sch√©mas de trafic pour optimiser la navigation et l‚Äôitin√©raire.
- Segmentation des donn√©es d‚Äôusage du v√©hicule pour des campagnes marketing cibl√©es.

---
## ML : Apprentissage par renforcement

<div style="display: flex; align-items: center; gap: 20px;">
    <div style="flex: 1;">
Agents apprenant par essais et erreurs

Syst√®mes de r√©compense
    </div>

    <div style="flex: 1;">

<iframe width="560" height="315" src="https://www.youtube.com/embed/spfpBrBjntg?si=68Z-oEMzvfxk8p6x&autoplay=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

    </div>

</div>
---

## <div style="display: flex; justify-content: center; align-items: center; height: 100vh; text-align: center; font-size: 10vw; font-weight: bold; width: 100%;">Maitre Corbeau sur un arbre ?</div>

---

## Maitre Corbeau sur un arbre perch√©
http://andreetgyps.a.n.pic.centerblog.net/o/6b0e0247.jpg

---
## Tokenisation

Les tokens en traitement du langage naturel (NLP) sont comme les syllabes en po√©sie. Tout comme les syllabes sont les √©l√©ments constitutifs du rythme et de la structure d'un po√®me, les tokens sont les unit√©s fondamentales qui permettent aux mod√®les d'IA de traiter et de comprendre le texte.

## "Maitre Corbeau sur un arbre perch√©" ‚Üí d√©casyllabe
- **Syllabes dans un po√®me :** Maitre Corbeau sur un arbre perch√©.
- **Tokens en NLP :** [Mai ##tre Cor ##beau sur un ar### bre perch√©.].

---

## Token dans les Mod√®les d'IA

La limite de tokens d√©finit le nombre maximum de tokens qu'un mod√®le peut traiter dans une seule entr√©e. Des limites de tokens plus √©lev√©es permettent de g√©rer des contextes plus longs, rendant les mod√®les plus efficaces pour des t√¢ches comme la synth√®se, l'analyse de code et la g√©n√©ration de documents.

| Mod√®le         | Taille Max (tokens) | Pages Livre de Poche Approx. |
|----------------|---------------------|------------------------------|
| GPT-5          | 128 000             | ~512                         |
| Llama 3.1      | 128 000             | ~512                         |
| Mistral Large  | 64 000              | ~256                         |

---

## Embedding

## Transformer les Tokens en Repr√©sentations Num√©riques

<div style="display: flex; align-items: center; gap: 20px;">

    <div style="flex: 1;">

L'embedding transforme les tokens en vecteurs, qui servent de v√©ritables points d'entr√©e pour le LLM.

    </div>

    <div style="flex: 1;">

![Exemple d'Embedding](https://causewriter.ai/wp-content/uploads/2023/08/image-2.png)

    </div>
</div>

---

## Comment la Tokenisation et l'Embedding Fonctionnent Ensemble :
**Tokenisation :**
- Divise le texte en tokens (par exemple, mots, sous-mots ou caract√®res).
- Exemple : "Maitre Corbeau sur un arbre perch√©" ‚Üí [Mai ##tre Cor ##beau sur un ar### bre perch√©.].

**Embedding :**
- Associe chaque token √† un vecteur de haute dimension dans un espace continu.
- Exemple : [Mai ##tre Cor ##beau sur un ar### bre perch√©.]. ‚Üí [[0.12, 0.45, ...], [0.34, 0.67, ...], [0.89, 0.23, ...]].

---

## Pourquoi l'Embedding est Important :
- **Compr√©hension S√©mantique :** Les tokens ayant des significations similaires ont des embeddings plus proches dans l'espace vectoriel.


```mermaid
graph LR
  A["Input Phrase: 'Maitre Corbeau sur un arbre perch√©'"] --> B["Tokenization: [Mai ##tre Cor ##beau sur un ar### bre perch√©.]"]
  B --> C["Embedding: Dense Numerical Vectors"]

  C["Tokenization Output"]
  C --> D["Token: 'Mai'"]
  D --> D1["Vector: [0.12, 0.45, 0.78, ...]"]
  C --> E["Token: '##tre'"]
  E --> E1["Vector: [0.34, 0.67, 0.89, ...]"]
  C --> F["Token: 'Cor'"]
  F --> F1["Vector: [0.56, 0.23, 0.91, ...]"]
  C --> G["Token: '##beau'"]
  G --> G1["Vector: [0.78, 0.12, 0.34, ...]"]
  C --> H["Token: 'sur'"]
  H --> H1["Vector: [0.45, 0.89, 0.67, ...]"]
  ```

---

---

## Param√®tres et Poids dans les R√©seaux de Neurones

Dans les r√©seaux de neurones, les **param√®tres** font r√©f√©rence aux valeurs ajustables que le mod√®le apprend pendant l'entra√Ænement. Ceux-ci incluent :

**Poids :**

- Repr√©sentent la force de la connexion entre les neurones.
- Ajust√©s pendant l'entra√Ænement pour minimiser l'erreur entre les sorties pr√©dites et r√©elles.

**Biais :**
- Ajout√©s √† la somme pond√©r√©e des entr√©es pour d√©caler la fonction d'activation.
- Aident le mod√®le √† mieux s'adapter aux donn√©es en permettant une flexibilit√© dans les fronti√®res de d√©cision.

---

## Param√®tres et Poids dans les R√©seaux de Neurones

<div style="display: flex; align-items: center; gap: 20px;">
    <div style="flex: 1;">

**Pourquoi Ils Sont Importants :**

- **Les poids et les biais** sont les composants essentiels qui permettent aux r√©seaux de neurones d'apprendre des motifs et de faire des pr√©dictions. En mettant √† jour de mani√®re it√©rative ces valeurs √† l'aide d'algorithmes d'optimisation comme la descente de gradient, le r√©seau am√©liore ses performances sur la t√¢che donn√©e.
    </div>

    <div style="flex: 1;">
**Exemple :**

- Dans un r√©seau de neurones simple, si l'entr√©e est `X`, le poids est `W` et le biais est `B`, la sortie d'un neurone est calcul√©e comme :
$$\text{sortie} = \text{fonction\_activation}(W \cdot X + B)$$

    </div>
</div>


---

## Mistral 7B : Nombre de Param√®tres

<div style="display: flex; align-items: center; gap: 20px;">

    <div style="flex: 1;">

Le mod√®le Mistral 7B est un mod√®le de fondation de pointe avec **7 milliards de param√®tres**.


**Comparaison :**
- **GPT-4 :** Environ 175 milliards de param√®tres estim√©s.
- **LLaMA 2 (13B) :** 13 milliards de param√®tres.
- **GPT-5 : ** entre 500 et 1500 milliards de param√®tres selon les rumeurs.
    </div>

    <div style="flex: 1;">

![Comparaison des Param√®tres de Mod√®les](https://www.geeky-gadgets.com/wp-content/uploads/2023/09/New-Mistral-7B-instruct-model-from-Mistral-AI.webp)

    </div>
</div>

---

## LLMs Un mod√®le avec beaucoup beaucoup ... beaucoup de param√®tres
LLM: Large Language Model

---
## ML : Apprentissage par Renforcement‚Äã

<div style="display: flex; align-items: center; gap: 20px;">
    <div style="flex: 1;">
Agents apprenant par essais et erreurs‚Äã

Syst√®mes de r√©compense‚Äã
    </div>

    <div style="flex: 1;">

<iframe width="560" height="315" src="https://www.youtube.com/embed/spfpBrBjntg?si=68Z-oEMzvfxk8p6x&autoplay=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

    </div>

</div>

---

## ML : Apprentissage par Renforcement‚Äã
<div style="display: flex; align-items: center; gap: 20px;">
    <div style="flex: 1;">

- Syst√®mes de conduite autonome apprenant des strat√©gies de conduite optimales par simulation.
- Syst√®mes de r√©gulateur de vitesse adaptatif optimisant l'efficacit√© √©nerg√©tique et la s√©curit√©.
- Syst√®mes d'assistance au stationnement apprenant √† naviguer dans des sc√©narios de stationnement complexes.

    </div>
    <div style="flex: 1;">  

<iframe width="560" height="315" src="https://www.youtube.com/embed/KPLYhRBCcvk?autoplay=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

**Source :** [AlphaStar : Niveau grand ma√Ætre dans StarCraft II utilisant l'apprentissage par renforcement multi-agents](https://deepmind.google/discover/blog/alphastar-grandmaster-level-in-starcraft-ii-using-multi-agent-reinforcement-learning/)

    </div>
</div>
---

## R√©seau de Neurones en Action

<div style="display: flex; align-items: center; gap: 20px;">

    <div style="flex: 1;">

L'animation montre le processus de reconnaissance de chiffres manuscrits √† l'aide d'un r√©seau de neurones. 

Elle visualise comment le mod√®le traite les images d'entr√©e, extrait les caract√©ristiques et pr√©dit le chiffre correspondant.

    </div>

    <div style="flex: 1;">

![R√©seau de Neurones en Action](https://i.makeagif.com/media/3-22-2022/boUeR6.gif)

    </div>
</div>

---

## Les Successeurs des LLM (vision de Yann Le Cun)
- JEPA : apprentissage auto-supervis√© qui pr√©dit des parties manquantes en espace de repr√©sentations (pas d‚Äôautocompl√©tion de tokens).
- World Models : mod√®les pr√©dictifs du monde (vid√©o/audio/action) pour raisonner et planifier.

---

# Comment l'Homme se positionne vis √† vis de l'IA ?

---

<!-- .slide: data-layout="text-image" data-img="https://image1.slideserve.com/2915781/brain-size-in-mammals-l.jpg" data-alt="Taille du cerveau chez les mammif√®res" -->

## üß† Le cerveau humain
- Taille moyenne : 1300 cm¬≥
- N√©andertal : 1600 cm¬≥
- Limit√© pour nouveaux neurones
- Synapses : pratiquement illimit√©es


---


## ‚ö° √ânergie : cerveau vs IA
| Syst√®me | Consommation |
|---------|--------------|
| Cerveau humain | ~20 W |
| Cluster d'IA | 1000 MW |

Le cerveau humain est aujourd'hui **100 millions de fois plus √©conome en √©nergie** qu'une IA.


## Comparaison des performances de l'IA et des humains
![Comparaison des performances de l'IA et des humains ‚Äî Our World in Data](https://upload.wikimedia.org/wikipedia/commons/1/11/Comparaison_des_performances_de_l%27IA_et_des_humains_-_Our_World_in_Data.svg?download)

---

# Et Demain ?

---

## ü§ñ L‚ÄôAGI : qu‚Äôest-ce que c‚Äôest ?
- AGI = **Artificial General Intelligence**
- IA capable de comprendre, apprendre et agir **comme un humain**
- Contrairement √† l‚ÄôIA actuelle, qui est sp√©cialis√©e

---

## üß† Diff√©rence IA sp√©cialis√©e vs AGI
| IA sp√©cialis√©e | AGI |
|----------------|-----|
| Fait une seule t√¢che | Peut apprendre toutes les t√¢ches |
| Exemple : AlphaGo | Exemple : r√©soudre un probl√®me, cr√©er, planifier |
| Limit√© √† un domaine | Flexible et g√©n√©raliste |

---

## üîÆ Vers l‚ÄôAGI
- Combinaison :
  - R√©seaux profonds
  - M√©moire et planification
  - Compr√©hension du langage et raisonnement
- Objectif : IA **polyvalente et autonome**

---

## üåå Pourquoi l‚ÄôAGI est fascinante
- Potentiel √©norme : science, m√©decine, exploration spatiale
- Risques : contr√¥le, √©thique, emploi
- Question cl√© : que se passe-t-il quand l'IA devient **plus intelligente que nous** ?

---
<!-- .slide: data-layout="two-column" data-img="https://media.licdn.com/dms/image/D4D12AQGyGyJI7Ht9fw/article-cover_image-shrink_600_2000/0/1668837711973?e=2147483647&v=beta&t=aGrDI4vzxLQz976zymt5s0DWTCp6GeG6UXLtKzmOxns" -->

## Singularit√© technologique en IA
- Point hypoth√©tique o√π l‚ÄôIA d√©passe l‚Äôintelligence humaine et s‚Äôauto-am√©liore rapidement.
- Timeline incertaine; sc√©nario graduel ou abrupt.


---

# Un futur pas si lointain ... 

---

D√®s 2027 on ne pourra pas faire la disctinction entre un humain et une IA

---
<!-- .slide: data-layout="two-column" data-img="https://www.eoi.es/blogs/redinnovacionEOI/files/2015/08/Jan2V_Film_Her.jpg" -->
## üé¨ Her (2013) avec Joaquin Phoenix
- Relation intime entre un homme et une IA avanc√©e
- Th√®mes: attachement √©motionnel, empathie simul√©e, solitude, autonomie de l‚ÄôIA.

---
<!-- .slide: data-layout="two-column" data-img="https://static.milibris.com/thumbnail/issue/1a2ac231-e35e-41be-a8f2-e3bf978e79a7/front/catalog-cover-large.jpeg" -->

## 2026 la une du magazine Lib√©ration du 22 janvier 2026

Le test de Turing est officiellement pass√© par toute IA grand public qui tient aujourd'hui dans votre poche.


---

<!-- .slide: data-layout="two-column" data-img="https://img-api.mac4ever.com/1179/0/af7e438e7e_qu-est-ce-que-le-slop-ce-curieux-phenomene-envahissant.webp" -->

## IA Slop ou la bouillie g√©n√©r√©e par IA qui pollue le web
- Contenu g√©n√©r√© automatiquement de faible qualit√©, produit √† grande √©chelle pour le clic/SEO, avec peu de valeur ajout√©e ou v√©rification.
- Impacts: pollution informationnelle, d√©sinformation, baisse de confiance, mod√®les r√©entra√Æn√©s sur donn√©es contamin√©es.

> "It‚Äôs becoming harder to detect what‚Äôs real and what‚Äôs AI-generated. This is particularly critical when it comes to deepfakes." le mercredi 21 janvier 2026.
By Neal Mohan, CEO, YouTube

---

<!-- .slide: data-layout="two-column" data-img="https://controverity.com/wp-content/uploads/2026/01/elon2026-1024x536.webp" -->

## La singularity arrive avec 30 ans d'avance

> "2026 est l'ann√©e de la singularit√© technologique."

Elon Musk le 4 janvier 2026


---

<!-- .slide: data-layout="two-column" data-video="https://youtube.com/shorts/zGfac0-MY20?si=c8Tn2hH6gn0-aaaG" -->
## L'IA est elle meilleure que nous ?

- Math√©maticien fran√ßais, m√©daille Fields 2010 pour ses travaux en th√©orie cin√©tique (√©quations de Boltzmann et de Landau).
- Auteur du rapport national 2018 sur l‚ÄôIA ‚ÄúDonner un sens √† l‚Äôintelligence artificielle‚Äù (strat√©gie fran√ßaise et europ√©enne).
- Ancien directeur de l‚ÄôInstitut Henri-Poincar√© et d√©put√©, engag√© sur sciences, √©ducation et innovation.

---

## la  course vers l'ia entretenue par les tensions geopolitiques entre les blocs USA CHINE EUROPE

---


## üñ•Ô∏è D√©monstration : Moshi de Kuytai
- G√©n√©ration de texte et images
- Interaction avec le public
- Illustrer puissance et limites de l‚ÄôIA


---

## Base de Donn√©es MNIST : Reconnaissance de Chiffres Manuscrits

<div style="display: flex; align-items: center; gap: 20px;">

    <div style="flex: 1;">

La base de donn√©es MNIST (Modified National Institute of Standards and Technology) est un benchmark largement utilis√© en apprentissage automatique et en vision par ordinateur. Elle se compose de 70 000 images en niveaux de gris de chiffres manuscrits (0 √† 9), chacune de taille 28x28 pixels. La base de donn√©es est utilis√©e pour entra√Æner et √©valuer des mod√®les pour des t√¢ches de reconnaissance de chiffres.

**Importance :**
- MNIST sert de point de d√©part pour tester et comparer les algorithmes d'apprentissage automatique.
- Elle aide √† comprendre comment les r√©seaux de neurones peuvent classifier des nombres bas√©s sur des motifs de pixels.

**Historique :**
- MNIST a √©t√© introduite en 1998 par **Yann LeCun, Corinna Cortes et Christopher J.C. Burges** dans le cadre de leurs recherches sur les r√©seaux de neurones et l'apprentissage automatique.

**Applications :**
- Reconnaissance de chiffres dans les syst√®mes postaux.
- Exp√©riences fondamentales en apprentissage profond.

    </div>
    <div style="flex: 1;">


![Exemple de la Base MNIST](https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png)

Exemple de reconnaissance de chiffres manuscrits de la base de donn√©es MNIST

    </div>
</div>


---

## Le test de Turing
**Principe :**
- Test propos√© par Alan Turing en 1950
- Une machine peut-elle convaincre un humain qu'elle est elle-m√™me humaine lors d'une conversation ?

---

<!-- .slide: data-layout="two-column" data-img="https://media.geeksforgeeks.org/wp-content/uploads/Turing-Diagram-159676.png" -->

## Comment √ßa marche :
1. Un humain dialogue avec deux interlocuteurs cach√©s
2. L'un est une machine, l'autre un humain
3. Si l'humain ne peut pas distinguer qui est qui, la machine "r√©ussit" le test
---

## Exemple concret :
- Vous discutez par √©crit avec deux personnes
- L'une parle de ses vacances, l'autre aussi
- Laquelle est l'IA ? Si vous ne pouvez pas le dire, l'IA a r√©ussi !

---

## Atelier Interactif : Comment d√©tecter une image g√©n√©r√©e par IA ?


https://this-person-does-not-exist.com/en


---

## Parlons avec le G√©n√©ral de Gaulle
https://unmute.sh/

---

## Applications Concr√®tes
  - G√©n√©ration d'une image simple ("un chat dans un jardin")
  - G√©n√©ration d'un po√®me avec le public
-  Exemples du quotidien (10 min)
  - Reconnaissance vocale, recommandations, traduction automatique
-  D√©tecter l'IA (5 min)
  - "Cette image a-t-elle √©t√© g√©n√©r√©e par une IA ?" (quiz visuel)




---

## Questions Cl√©s
- "L'IA est-elle plus intelligente que nous ?"
- "Peut-on lui faire confiance ?" 
- "Va-t-elle nous remplacer ?"